{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-biography",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp synchro.nested_stims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-equipment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-houston",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-shark",
   "metadata": {},
   "source": [
    "synchro.nested_stims\n",
    "\n",
    " >Functions to synchronize data from QDSpy stimuli that consist of presenting a sequence\n",
    " >of shorter stimuli. Often those are compiled shortly before their presentation so they\n",
    " >need different handling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-fourth",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from theonerig.synchro.io import *\n",
    "from theonerig.synchro.processing import *\n",
    "from theonerig.synchro.extracting import *\n",
    "\n",
    "from theonerig.core import *\n",
    "from theonerig.utils import *\n",
    "from theonerig.database import *\n",
    "from theonerig.processing import *\n",
    "from theonerig.plotting import *\n",
    "\n",
    "import numpy as np\n",
    "import glob, os, datetime\n",
    "import ast\n",
    "\n",
    "def get_stim_ids(log, record_time, lastmodif_time):\n",
    "    \"\"\"Obtain the correct name of the stimulus, based on the name that appears in the QDSpy log\n",
    "    \n",
    "    This is needed if no central database of compiled stimuli is used, but instead stimuli are compiled in QDSpy\n",
    "    every time before they are presented. \n",
    "   \n",
    "    params:\n",
    "        - log: log object generated by get_QDSpy_logs\n",
    "        - record_time: the stimulus presentation time, as calculated by subtracting stimulus duration from lastmodif_time\n",
    "        - lastmodif_time: the timestamp of the .npy file (time of compilation, assumed to be only seconds before record_time)\n",
    "    returns:\n",
    "        - stim_ids: Correct stim_id for the stimulus presented after the current log stimulus start       \n",
    "    \"\"\"\n",
    "    stim_ids = []\n",
    "    photo_early_end = []\n",
    "    for i, stim in enumerate(log.stimuli):\n",
    "        if stim.stop_time is None:\n",
    "            stim.stop_time = log.stimuli[i + 1].start_time\n",
    "        if (stim.start_time > record_time) & (stim.stop_time < lastmodif_time):\n",
    "            stim_ids.append(stim)\n",
    "        elif (stim.start_time > record_time):\n",
    "            photo_early_end.append(stim)\n",
    "    if len(stim_ids)==0:\n",
    "        print(\"Check the timestamp of the corresponding Photodiode file. The diode seems to have stopped before the stim ended.\")\n",
    "        print(\"No matching stimulus. Taking first stim after start of photodiode. Check timestamps carefully.\")\n",
    "        stim_ids = [photo_early_end[0]]\n",
    "    return stim_ids\n",
    "\n",
    "def get_synced_file(stim_list_dir, stim_id):\n",
    "    ''' \n",
    "        Find the stimulus in the stimulus list directory that is temporally closest to the\n",
    "        stimulus in the log. Works based on the modification time of the stimulus (i.e. \n",
    "        expects stimulus to be compiled shortly before display). \n",
    "        \n",
    "        params:\n",
    "            - stim_list_dir: fullpath to stimuli, string\n",
    "            - stim_id: stimulus read from QDSpy log, theonerig.synchro.extracting.Stimulus \n",
    "                        object\n",
    "        returns: \n",
    "            - stim: filename of the stimulus that needs loading, str\n",
    "    '''\n",
    "    stims = {\"stim_name\": [], \"stim_delta\": []}\n",
    "    for stim_list in os.listdir(stim_list_dir):\n",
    "        stim_load = datetime.datetime.fromtimestamp(int(os.path.getmtime(os.path.join(stim_list_dir, stim_list))))\n",
    "        stim_present = stim_id.start_time\n",
    "        # If the stimulus was compiled before display calculate difference, otherwise set to max\n",
    "        stim_delta = stim_present - stim_load if stim_present > stim_load else datetime.timedelta.max\n",
    "        stims[\"stim_name\"].append(stim_list)\n",
    "        stims[\"stim_delta\"].append(stim_delta)\n",
    "    # Obtain the index of the compiletime closest to the stimulus display time\n",
    "    closest_stim_idx = stims[\"stim_delta\"].index(min(stims[\"stim_delta\"]))\n",
    "    stim_fn = stims[\"stim_name\"][closest_stim_idx]\n",
    "    stim_path = os.path.join(stim_list_dir, stim_fn)\n",
    "    # Sanity check\n",
    "    if not stim_id.name in stim_path:\n",
    "        print(\"Compiled stimulus file not matching this name\")\n",
    "        print(\"stim_id: {}\".format(stim_id.name))\n",
    "        print(\"stimulus file: {}\".format(os.path.basename(stim_path)))\n",
    "        stim_path = os.path.join(stim_list_dir, os.path.basename(stim_path))\n",
    "        # Clean up: Ugly 1-time fix\n",
    "        if \"npz\" in stim_path:\n",
    "            stim = np.load(stim_path)\n",
    "            stim = stim['stim' + stim_id.filename[3:6]].flatten()\n",
    "        else:\n",
    "            stim = np.load(stim_path)\n",
    "    else:\n",
    "        stim = np.load(stim_path)\n",
    "\n",
    "    # Some of the stimuli have a shape of repetition_number x stim_onset:\n",
    "    if len(stim.shape) > 1:\n",
    "        stim = stim.flatten()\n",
    "    return stim, stim_path\n",
    "\n",
    "def get_synced_file_precompile(stim_list_dir, stim_id):\n",
    "    for stim_list in os.listdir(stim_list_dir):\n",
    "        if stim_id.md5 in stim_list:\n",
    "            stim_path = os.path.join(stim_list_dir, stim_list)\n",
    "            stim = np.load(stim_path)\n",
    "    # Some of the stimuli have a shape of repetition_number x stim_onset:\n",
    "    if len(stim.shape) > 1:\n",
    "        stim = stim.flatten()\n",
    "    return stim, stim_path\n",
    "\n",
    "def get_stim_dict(stim_dict_path, stim_filename):\n",
    "    \"\"\"Get the correct dictionary file that decodes the ints that encode the stimulus sequence. \n",
    "    Applicable to nested stimuli, whose _intensity_.npy file contains only a list of integers.\n",
    "    \n",
    "    params:\n",
    "        - stim_dict_path: [string], the path to the directory where all dicts are stored\n",
    "        - stim_filename: [string], the name of the stimulus, make sure that stim_id from the QDSpy log and the name of the dictionary match\n",
    "    \n",
    "    \"\"\"\n",
    "    for dict in os.listdir(stim_dict_path):\n",
    "        if stim_filename[0:6] in dict:\n",
    "            with open(os.path.join(stim_dict_path, dict),'r') as inf:\n",
    "                dict_from_file = []\n",
    "                for line in inf:\n",
    "                    dict_from_file.append(ast.literal_eval(line))   \n",
    "            return dict_from_file\n",
    "        \n",
    "def cluster_by_list(data, frame_timepoints, frame_signals, stim_list):\n",
    "    \"\"\"Assign the stimulus identity values from stim_list to the frames in data. stim_list contains only the\n",
    "        sequence of stimuli. Those need to be expanded. Opposed to cluster_frame_signals and cluster_by_epochs no\n",
    "        AUC operation is performed.\n",
    "        params:\n",
    "            - data: raw data used to compute the stimulus times\n",
    "            - frame_timepoints: timepoints delimiting each frame\n",
    "            - frame_signals: binary 1-D numpy array indicating if high_threshold was passed in 'detect_frames'\n",
    "            - stim_list: 1-D numpy array containing the sequence of the stimuli presented\n",
    "        returns:\n",
    "            - frame_signals: [same size as frame_timepoints] stim_signals list containing the correct value from\n",
    "                stim_list at every entry\"\"\"\n",
    "    epoch_end, stim_change = find_stim_onsets(frame_signals)\n",
    "    # Split into on times & values vs off times & values\n",
    "    stim_ons = stim_change[0::2]\n",
    "    #stim_ons_idx = stim_idx[0::2]\n",
    "    stim_offs = stim_change[1::2]\n",
    "    #stim_offs_idx = stim_idx[1::2]\n",
    "\n",
    "    # Replace the frame_signal entries with the stimulus codes\n",
    "    frame_signals[frame_signals == 0] = -1 # To avoid confusion with the '0' stimulus code\n",
    "    for i,stim_type in enumerate(stim_list):\n",
    "        if i >= len(stim_ons):\n",
    "            print(\"WARNING: Not all stimuli of the stim_list were actually presented. Recording ended early.\")\n",
    "            frame_signals[stim_ons[i - 1:]] = -1\n",
    "            return frame_signals, stim_ons, stim_offs, epoch_end\n",
    "        frame_signals[stim_ons[i]:stim_offs[i]] = stim_type\n",
    "\n",
    "    return frame_signals, stim_ons, stim_offs, epoch_end\n",
    "\n",
    "def find_stim_onsets(frame_signals):\n",
    "    \"\"\"Get the frame timepoints during which the stimuli started and ended. \n",
    "    Crop out the last two ones that are currently just an offset marker.\n",
    "    \n",
    "    params:\n",
    "        - frame_signals: processed signal from the Photodiode files, containing the marker information\n",
    "    returns:\n",
    "        - epoch_end: the frame timepoints of the offset signal\n",
    "        - stim_change: the frame timepoints of every stimulus on and offset\n",
    "    \"\"\"\n",
    "    stim_change = np.where(frame_signals[:-1] != frame_signals[1:])[0]\n",
    "    stim_change = stim_change + 1 # since I need to compare to [1:] all values are shifted by 1\n",
    "    #stim_idx = frame_timepoints[stim_change]\n",
    "\n",
    "    # QDSpy currently is set to emit a short peak at the end to indicate the end of the stimulus presentation\n",
    "    # This peak needs to be ignored\n",
    "    epoch_end = stim_change[-2:] # return it for future analysis\n",
    "    stim_change = stim_change[:-2] #add it to the no stimulus category\n",
    "    return epoch_end, stim_change\n",
    "\n",
    "def append_tread_movement(reM, epoch_counter, treadmill_data, ref_timepoints):\n",
    "    '''\n",
    "    Append treadmill data to the record Master\n",
    "    '''\n",
    "    tread_tp = np.arange(len(treadmill_data), dtype=\"int\")\n",
    "    tread_downsamp = resample_to_timepoints(tread_tp, treadmill_data, ref_timepoints)\n",
    "    reM[epoch_counter - 1][\"treadmill\"] = tread_downsamp\n",
    "    return reM\n",
    "\n",
    "def twoP_dataChunks_2(ref_timepoints, frame_timepoints, len_epochs, cursor, skip=None, popped_epoch=None, *args):\n",
    "    \"\"\"\n",
    "    Factory function for two photon data. \n",
    "\n",
    "    params:\n",
    "        - ref_timepoints: Reference timepoints to create the DataChunk\n",
    "        - frame_timepoints: List of frame timepoints for each sequence of two photon frame recorded.\n",
    "        - len_epochs: Lenght of the recorded epochs (<= than the corresponding frame_timepoints). Int or list\n",
    "        - args: matrices of all frames detected by CaImAn. (give as many as you want to synchronise)\n",
    "\n",
    "    return:\n",
    "        - tuple containing the synchronised matrices in the order it was given\n",
    "    \"\"\"\n",
    "    assert len(args)>=1, \"no matrix to be synchronised was given\"\n",
    "    res_l = [[] for i in range(len(args))] # A list containing 1 entry per Ca-matrix to be processed\n",
    "    if cursor is None:\n",
    "        cursor = 0 # counter of from where to start indexing into the Ca-matrices\n",
    "    if isinstance(len_epochs, int):\n",
    "        len_epochs = [len_epochs]\n",
    "    if isinstance(popped_epoch, int):\n",
    "        popped_epoch = [popped_epoch]\n",
    "    # For every recording block (defined by len_epochs, that counts the number of frames within \n",
    "    # the recording), find the index (for the ref_timepoints) of the first and last frame of \n",
    "    # that block.\n",
    "    for i, len_epoch in enumerate(len_epochs):\n",
    "        print(i)\n",
    "        if i == 0:\n",
    "            set_trace()\n",
    "        first_ca = frame_timepoints[i][0]\n",
    "        if len_epoch > len(frame_timepoints[i]):\n",
    "            last_ca = frame_timepoints[i][-1]\n",
    "            print(i, \"Ca data longer than sync timepoints!\")\n",
    "        else:\n",
    "            last_ca = frame_timepoints[i][len_epoch-1]\n",
    "        if isinstance(ref_timepoints, Data_Pipe):\n",
    "            start_idx = np.argmax(ref_timepoints[i]['main_tp']>first_ca)\n",
    "            stop_idx  = np.argmax(ref_timepoints[i]['main_tp']>last_ca)\n",
    "        else:\n",
    "            start_idx = np.argmax(ref_timepoints>first_ca)\n",
    "            stop_idx  = np.argmax(ref_timepoints>last_ca)\n",
    "        for k, matrix in enumerate(args):\n",
    "            # Slice the Ca-matrix in the time dimension to the duration of the recording block\n",
    "            sub_mat = matrix.T[cursor:cursor+len_epoch]\n",
    "            # Generate a linear interpolation function of the Ca-matrix values over the time of the\n",
    "            # recording block, axis 0 is axis of the Ca-intensity values over time \n",
    "            f = interpolate.interp1d(range(len_epoch), sub_mat, axis=0)\n",
    "            # stop_idx-start_idx defines the number of timepoints for which values need to be \n",
    "            # interpolated (i.e resolution). The block duration is again defined by len_epoch.\n",
    "            # This interpolated data will be inserted as a datachunk object starting at the \n",
    "            # correct start_idx (indexing ref_timpoints)\n",
    "            res_l[k].append(DataChunk(data=f(np.linspace(0,len_epoch-1,stop_idx-start_idx)), \n",
    "                                           idx=start_idx, \n",
    "                                           group=\"cell\"))\n",
    "        if (skip is not None) and (i+1 in [skip]):\n",
    "            cursor += len_epoch + popped_epoch[0] # skip the 3000 water frames\n",
    "            del popped_epoch[0]\n",
    "        else:\n",
    "            cursor += len_epoch # Skip indexing to the start of the next recording epoch\n",
    "    return tuple(res_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-barcelona",
   "metadata": {},
   "source": [
    "# ReM generation loops\n",
    "Always make sure that the alignment is correct using ```find_starting_position.ipynb```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-hayes",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from theonerig.synchro.io import *\n",
    "from theonerig.synchro.processing import *\n",
    "from theonerig.synchro.extracting import *\n",
    "\n",
    "from theonerig.core import *\n",
    "from theonerig.utils import *\n",
    "from theonerig.database import *\n",
    "from theonerig.processing import *\n",
    "from theonerig.plotting import *\n",
    "\n",
    "import numpy as np\n",
    "import glob, os, datetime\n",
    "def append_to_reM(epoch_counter, reM, retain_template, stim_list_dir, stim_ids, record_time, \n",
    "                                photo_data, stim_code_dict, frame_tp, ref_timepoints, \n",
    "                               ref_signals, tom_stim_dir):\n",
    "    for stim in stim_ids:\n",
    "    # Can't: Lerion didn't save it there. obtaining stimulus parameters from the database\n",
    "    #     stim_params = stim_param_to_dict(stim_params_df, stim.md5)\n",
    "    #     first_fr, last_fr = stim_params[\"first_frame_id\"], stim_params[\"last_frame_id\"]\n",
    "\n",
    "        # One of Tom's stimuli??\n",
    "        if stim.name in retain_template:\n",
    "#             tom_stim_dir = os.path.join(\"/media/asari/Tristan/01_Code/013_Leiron_Sweep/Day_1\")\n",
    "            # unpacked is a tuple of (frames presented, REF and shader)\n",
    "            unpacked = unpack_stim_npy(tom_stim_dir, stim.md5)\n",
    "            estimate_start = get_position_estimate(stim.start_time, record_time, 10000)\n",
    "            if estimate_start<0:\n",
    "                print(stim, \"Started before the record, passing\")\n",
    "                continue\n",
    "            # Needs to be adjusted manually, open photo_data plots and read out first\n",
    "            # and last frames. Find based on ref_tp alignment (REC) with unpacked[1] (REF)\n",
    "            starting_pos = input(\"This stimulus needs to be synchronized. Please enter the starting ref_tp: \")\n",
    "            starting_pos = int(starting_pos)\n",
    "            signals = ref_signals[starting_pos:starting_pos+len(unpacked[1])]\n",
    "            stim_tuple, shift_log, frame_replacement  = frame_error_correction(signals,\n",
    "                                                                                unpacked,\n",
    "                                                                                algo=\"conv\")\n",
    "            print(\"\\n\",stim.filename, \"match at\",starting_pos)\n",
    "            display_match(starting_pos, unpacked[1], ref_signals, stim_tuple[1])\n",
    "            \n",
    "            # For Tom's stimuli: The first presentations are short burst encoding the stimulus\n",
    "            # & the last one is a peak shorter than the rest. I'm trying to crop those out below\n",
    "            # This needs to be verified later\n",
    "#             epoch_end, stim_changes = find_stim_onsets(ref_signals)\n",
    "#             last_frame =  epoch_end[0]\n",
    "            \n",
    "#             stim_duration = np.diff(stim_changes)\n",
    "            first_frame = input(\"Please enter the first ref_tp of the first stimulus after the prelude code: \")\n",
    "            first_frame = int(first_frame)\n",
    "            last_frame = input(\"Please enter the last ref_tp of the first stimulus before the end-of-stimulus epoch: \")\n",
    "            last_frame = int(last_frame)\n",
    "            stim_tuple, shift_log, frame_replacement = chop_stim_edges(first_frame, last_frame, \n",
    "                                                   stim_tuple, shift_log, frame_replacement)\n",
    "            \n",
    "            idx_stim_val = 0\n",
    "            # If the photodiode data was presented in the lower right corner of the screen set '0'\n",
    "            ud_inv = lr_inv = 0\n",
    "#             if stim_tuple[2] is not None:\n",
    "#                 idx_stim_val = 2 #If there is a shader, we take the shader instead of the intensity\n",
    "            if stim.name == \"moving_gratings\":\n",
    "                flipped_inten = flip_gratings(stim_tuple[idx_stim_val], ud_inv, lr_inv)\n",
    "            else:\n",
    "                flipped_inten = flip_stimulus(stim_tuple[idx_stim_val], ud_inv, lr_inv)\n",
    "            stim_datachunk = stim_to_dataChunk(flipped_inten, \n",
    "                                               starting_pos, #We sliced out the synchro elements of the stim\n",
    "                                               ref_signals)\n",
    "\n",
    "    #         stim_datachunk.attrs.update(stim_params) # only works if database data present\n",
    "            stim_datachunk.attrs[\"signal_shifts\"]     = shift_log\n",
    "            stim_datachunk.attrs[\"frame_replacement\"] = frame_replacement\n",
    "            stim_datachunk.attrs[\"md5\"]  = stim.md5\n",
    "            stim_datachunk.attrs[\"name\"] = stim.filename #Storing the name of the stim in the datachunck aswell\n",
    "\n",
    "            stim_key = stim.filename\n",
    "            reM[epoch_counter - 1][stim_key] = stim_datachunk\n",
    "\n",
    "\n",
    "        # One of Leiron's stimuli\n",
    "        else:\n",
    "            # 1) Get the stimulus list from the correct .py QDSpy file\n",
    "            stim_list, stim_path = get_synced_file_precompile(stim_list_dir, stim)\n",
    "            # 2) Expand the stimulus values to the actually presented frames\n",
    "            frame_signals_clustered, stim_ons, stim_offs, epoch_end = \\\n",
    "                    cluster_by_list(photo_data, ref_timepoints, ref_signals, stim_list)\n",
    "            i = 0\n",
    "            for onset, offset in zip(stim_ons, stim_offs):\n",
    "\n",
    "                # 3) Are not read out from the log, but from the photodiode data in this case\n",
    "                # first_fr is the first actual frame of stimulus presentation, in this case the stimulus starts immediately\n",
    "                first_fr = onset\n",
    "                last_fr = offset\n",
    "\n",
    "                signals = np.array(ref_signals[first_fr:first_fr+len(\n",
    "                                    frame_signals_clustered[first_fr:last_fr])])\n",
    "                # Error correction compares the data from the ref_signals with the assigned 'cluster'\n",
    "                # values. \n",
    "                stim_tuple, shift_log, frame_replacement  = frame_error_correction(signals,\n",
    "                                            [np.empty_like(signals), \n",
    "                                             frame_signals_clustered[first_fr:last_fr], \n",
    "                                            np.empty_like(signals)],\n",
    "                                                                            algo=\"conv\")\n",
    "                \n",
    "                \n",
    "#                 print(\"\\n\",stim.filename, \"match at\",first_fr)\n",
    "#                 display_match(first_fr, frame_signals_clustered[first_fr:last_fr], \n",
    "#                               ref_signals, corrected=None)\n",
    "            \n",
    "                stim_datachunk = stim_to_dataChunk(frame_signals_clustered[first_fr:last_fr], \n",
    "                                                   first_fr, #No syncro elements in this data\n",
    "                                                   ref_signals)\n",
    "\n",
    "                # 8) Add additional info from log file\n",
    "                stim_datachunk.attrs[\"md5\"]  = stim.md5\n",
    "                stim_datachunk.attrs[\"signal_shifts\"]     = shift_log\n",
    "                stim_datachunk.attrs[\"frame_replacement\"] = frame_replacement\n",
    "                \n",
    "                #Loading the stim dictionary and putting the correct names \n",
    "                stim_idx = np.unique(stim_datachunk.data)[0]\n",
    "                if stim_idx == -1:\n",
    "                    stim_datachunk.attrs[\"name\"] = \"not recorded, photodiode ended early\"\n",
    "                else:\n",
    "                    stim_datachunk.attrs[\"name\"] = stim_code_dict[stim_idx] #Storing the name of the stim in the datachunck aswell\n",
    "\n",
    "\n",
    "                # 9) Update the reM\n",
    "                stim_key =  stim_datachunk.attrs[\"name\"]\n",
    "                # Based on stim_key read meaning of stim_key from dictionary\n",
    "                reM[epoch_counter - 1][stim_key] = stim_datachunk\n",
    "                i += 1\n",
    "    return reM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from theonerig.synchro.io import *\n",
    "from theonerig.synchro.processing import *\n",
    "from theonerig.synchro.extracting import *\n",
    "\n",
    "from theonerig.core import *\n",
    "from theonerig.utils import *\n",
    "from theonerig.database import *\n",
    "from theonerig.processing import *\n",
    "from theonerig.plotting import *\n",
    "\n",
    "import numpy as np\n",
    "import glob, os, datetime\n",
    "\n",
    "def master_loop(log, retain_template, tom_stim_dir, sync_dir, calcium_dir, stim_list_dir, \n",
    "                stim_dict_dir, stack_info_dir):\n",
    "#     sync_dir = os.path.join(working_dir, \"sync/Session01\")\n",
    "    epoch_counter = 0\n",
    "    sync_files = np.sort(glob.glob(sync_dir + \"/Photo*\"))\n",
    "    l_epochs = stack_len_extraction(stack_info_dir)\n",
    "#     l_epochs[-1] = l_epochs[0] = 9033\n",
    "    # Files should be numbered from 00 to 99, extract that number \n",
    "    assert os.path.isdir(tom_stim_dir),      \"precompiled stim dir path does not exist\"\n",
    "    assert os.path.isdir(sync_dir),          \"sync dir path does not exist\"\n",
    "    assert os.path.isdir(calcium_dir),       \"calcium_data dir path does not exist\"\n",
    "    assert os.path.isdir(stim_list_dir),     \"stim_list dir path does not exist\"\n",
    "    assert os.path.isdir(stim_dict_dir),     \"stim_dict dir path does not exist\"\n",
    "    assert len(sync_files) > 0,              \"sync dir doesn't contain any Photodiode files\"\n",
    "    print(\"len sync_files: {}\".format(len(sync_files)))\n",
    "    print(\"len l_epochs: {}\".format(len(l_epochs)))\n",
    "#     assert len(sync_files) == len(l_epochs), \"number of sync files and recording epochs don't align\"\n",
    "\n",
    "#     # For this particular record\n",
    "    skip = [12,13]\n",
    "    skipped_epoch = []\n",
    "    for skips in reversed(skip):\n",
    "#     skip = 12 #water sync broken\n",
    "        skipped_epoch.append(l_epochs.pop(skips-1))\n",
    "    \n",
    "#     del l_epochs[skip]\n",
    "    \n",
    "    twoP_timepoints = []\n",
    "    for record_file in sync_files:\n",
    "        record_file_number = record_file.split('_')[-1]\n",
    "        epoch_counter_current = int(record_file_number)\n",
    "        \n",
    "        if (skip is not None) and (epoch_counter_current == skip):\n",
    "            print(\"Skipping recording #{}\".format(skip))\n",
    "            continue\n",
    "\n",
    "        # Maybe list all files using glob glob instead\n",
    "        if epoch_counter_current < epoch_counter:\n",
    "            break\n",
    "        else:\n",
    "            # 0) Update file counter\n",
    "            epoch_counter = epoch_counter_current\n",
    "            # 1) Load the data\n",
    "            photo_path = os.path.join(sync_dir,\"Photodiode_\" + record_file_number)\n",
    "            # Should be global\n",
    "#             stim_list_dir = os.path.join(\"/media/asari/Tristan/01_Code/013_Leiron_Sweep/Day_1\")           \n",
    "            print(\"Processing: \" + photo_path)\n",
    "            photo_data = load_sync_raw(photo_path)\n",
    "            cam_data = load_sync_raw(os.path.join(sync_dir,\"CamTrigs_\" + record_file_number))\n",
    "            treadmill_data = load_sync_raw(os.path.join(sync_dir,\"Treadmill_\" + \n",
    "                                                        record_file_number))\n",
    "            scanning2p_data = load_sync_raw(os.path.join(sync_dir,\"SyncPulses_\" + \n",
    "                                                         record_file_number))\n",
    "            \n",
    "#             calcium_dir = os.path.join(working_dir, \"calcium_data\")\n",
    "            twoP_timepoints.append(detect_calcium_frames(scanning2p_data)[0]) #MUST BE APPEND TO LIST\n",
    "            # 2) Get the timestamps for alignment\n",
    "            lastmodif_time = datetime.datetime.fromtimestamp(int(os.path.getmtime(photo_path)))\n",
    "            time_lenfile   = datetime.timedelta(0, int(len(photo_data)/10000)) \n",
    "            record_time = lastmodif_time-time_lenfile\n",
    "            date_str = str(record_time).split(\" \")[0]\n",
    "            # 3) Obtain the stimuli presented during the epoch\n",
    "            stim_ids = get_stim_ids(log, record_time, lastmodif_time)\n",
    "            # 4) Obtain the encoding of the stimuli       \n",
    "            print(stim_ids[0].filename)\n",
    "            if stim_ids[0].name in retain_template:\n",
    "                stim_code_dict = None\n",
    "            else:\n",
    "                stim_code_dict = get_stim_dict(stim_dict_dir, \n",
    "                                           stim_ids[0].filename)[0]\n",
    "            # Thresholds and sampling rate might need to be adjusted. Check photo_data\n",
    "            low_th = 0.1\n",
    "            if stim_ids[0].name == \"water\":\n",
    "                high_th = 0.6\n",
    "            else:\n",
    "                high_th = 1.2\n",
    "            frame_tp, frame_signals = detect_frames(photo_data, low_th, high_th, \n",
    "                                                    increment=int(10000/60))\n",
    "\n",
    "            ref_timepoints, ref_signals   = extend_sync_timepoints(frame_tp, frame_signals,\n",
    "                                                                   up_bound=len(photo_data))\n",
    "            if epoch_counter == 1:\n",
    "                reM = RecordMaster([(ref_timepoints, ref_signals)])\n",
    "            else:\n",
    "                reM.append(ref_timepoints, ref_signals)\n",
    "            # 6) Add all stimuli to the record Master\n",
    "            reM = append_to_reM(epoch_counter, reM, retain_template, stim_list_dir, stim_ids, record_time, \n",
    "                                photo_data, stim_code_dict, frame_tp, ref_timepoints, \n",
    "                               ref_signals, tom_stim_dir)\n",
    "            # 7) Add treadmill data to record Master\n",
    "            reM = append_tread_movement(reM, epoch_counter, treadmill_data, ref_timepoints)\n",
    "            \n",
    "    # 8) Add the calcium recording data\n",
    "    C_matrix = np.load(os.path.join(calcium_dir,\"C.npy\"))\n",
    "    S_matrix = np.load(os.path.join(calcium_dir,\"S.npy\"))\n",
    "    pipe = Data_Pipe(reM, [\"main_tp\"])\n",
    "    pipe += \"main_tp\"\n",
    "    ca2p_C_datachunks_l = []\n",
    "    ca2p_S_datachunks_l = []\n",
    "    ca2p_C_datachunks, ca2p_S_datachunks = twoP_dataChunks_2(pipe, \n",
    "                                                               twoP_timepoints, \n",
    "                                                               l_epochs, # compare to other script, shoul dbe - 1?\n",
    "                                                               0, #Grey was recorded, but excluded from C matrix generation\n",
    "                                                               skip,\n",
    "                                                               skipped_epoch,\n",
    "                                                               C_matrix,\n",
    "                                                               S_matrix)\n",
    "    for i, slc in enumerate(pipe._slices):   \n",
    "        # Add 1) Ca-data 2) movement info to every slice (i.e. Contiguous Record)\n",
    "        reM[slc[0]][\"C_matrix\"] = ca2p_C_datachunks[i]\n",
    "        reM[slc[0]][\"S_matrix\"] = ca2p_S_datachunks[i]\n",
    "            \n",
    "    # 9) Determine still and movement epochs\n",
    "    # Slice only the parts of the treadmill record when stimuli were presented\n",
    "    pipe = Data_Pipe(reM, [\"treadmill\"])\n",
    "    pipe += \"stim\"\n",
    "    \n",
    "\n",
    "    movement_threshold = 0.2\n",
    "    for data_d, slc in zip(pipe, pipe._slices):   \n",
    "        treadmill_dc = data_d[\"treadmill\"]\n",
    "        tread_mean_speed = np.array(treadmill_dc).mean() # Needs type conversion \n",
    "        \n",
    "        if np.abs(tread_mean_speed) > movement_threshold:\n",
    "            reM[slc[0]][\"moving\"] = DataChunk(np.array(treadmill_dc), idx=treadmill_dc.idx,\n",
    "                                             group=\"data\", fill=treadmill_dc.fill)\n",
    "        elif np.abs(tread_mean_speed) <= movement_threshold:\n",
    "            reM[slc[0]][\"still\"] = DataChunk(np.array(treadmill_dc), idx=treadmill_dc.idx, \n",
    "                                             group=\"data\", fill=treadmill_dc.fill)\n",
    "    \n",
    "    return reM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-pontiac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_utils.ipynb.\n",
      "Converted 02_processing.ipynb.\n",
      "Converted 03_modelling.ipynb.\n",
      "Converted 04_plotting.ipynb.\n",
      "Converted 05_database.ipynb.\n",
      "Converted 06_eyetrack.ipynb.\n",
      "Converted 10_synchro.io.ipynb.\n",
      "Converted 11_synchro.extracting.ipynb.\n",
      "Converted 12_synchro.processing.ipynb.\n",
      "Converted 13_leddome.ipynb.\n",
      "Converted 14_synchro.nested_stims.ipynb.\n",
      "Converted 99_testdata.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
