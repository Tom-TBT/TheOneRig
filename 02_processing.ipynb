{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# processing\n",
    "> Processing the different stream of data to calculate responses of retinal cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import cluster\n",
    "import scipy.ndimage as ndimage\n",
    "import scipy.signal as signal\n",
    "import scipy as sp\n",
    "from cmath import *\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "from theonerig.core import *\n",
    "from theonerig.utils import *\n",
    "from theonerig.modelling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def eyetrack_stim_inten(stim_inten, eye_track, \n",
    "                        upsampling=2,\n",
    "                        eye_calib=[[94 ,8], [ 18, 59]],\n",
    "                        box_w=None, box_h=None, stim_axis=\"x\"):\n",
    "    \"\"\"From stimulus data and eye tracking, returns a corrected and upsampled stimulus data.\"\"\"\n",
    "    eye_x, eye_y = eye_track[:,0], eye_track[:,1]\n",
    "    shape_y, shape_x = 1, 1\n",
    "    if len(stim_inten.shape)==2:\n",
    "        if stim_axis==\"x\":\n",
    "            shape_x = stim_inten.shape[1]\n",
    "        elif stim_axis==\"y\":\n",
    "            shape_y = stim_inten.shape[1]\n",
    "    elif len(stim_inten.shape)==3:\n",
    "        shape_y = stim_inten.shape[1]\n",
    "        shape_x = stim_inten.shape[2]\n",
    "    if box_w is None:\n",
    "        box_w = 1280//shape_x\n",
    "    if box_h is None:\n",
    "        box_h = 720//shape_y\n",
    "\n",
    "    if shape_y>1 and shape_x>1:\n",
    "        box_w, box_h = int(box_w/upsampling), int(box_h/upsampling)\n",
    "    elif shape_x > 1:\n",
    "        box_w, box_h = int(box_w/upsampling), box_h\n",
    "    elif shape_y > 1:\n",
    "        box_w, box_h = box_w                , int(box_h/upsampling)\n",
    "\n",
    "    eye_transfo_f = _eye_to_stim_f(eye_calib=eye_calib, \n",
    "                                  box_width=box_w,\n",
    "                                  box_height=box_h)\n",
    "    \n",
    "    if shape_y>1 and shape_x>1:\n",
    "        stim_inten = stim_inten.repeat(upsampling,axis=1).repeat(upsampling,axis=2)\n",
    "    else:\n",
    "        stim_inten = stim_inten.repeat(upsampling,axis=1)\n",
    "        \n",
    "    xpos_avg = np.mean(eye_x)\n",
    "    ypos_avg = np.mean(eye_y)\n",
    "    mean_stim_inten = int((np.max(stim_inten)+np.min(stim_inten))/2)\n",
    "    #After getting the shift of the matrix to apply, we roll the matrix instead of extending it to the shifts\n",
    "    #This seems strange, but from the cell point of view, that is potentially looking at no stimulus,\n",
    "    # the response it gives are uncorrelated with the stimulus, and so shouldn't impact further analysis\n",
    "    # Advantage is that it keeps the data small enough, without loosing regions of the stimulus.\n",
    "    for i in range(len(stim_inten)):\n",
    "        x_eyeShift = eye_x[i]-xpos_avg\n",
    "        y_eyeShift = eye_y[i]-ypos_avg\n",
    "        \n",
    "        stim_shift_x, stim_shift_y = eye_transfo_f(x_eyeShift=x_eyeShift,\n",
    "                                                   y_eyeShift=y_eyeShift) \n",
    "        if shape_y>1 and shape_x>1:\n",
    "            rolled_stim = np.roll(stim_inten[i],stim_shift_y,axis=0)\n",
    "            rolled_stim = np.roll(rolled_stim  ,stim_shift_x,axis=1)\n",
    "        else:\n",
    "            if stim_axis==\"x\":\n",
    "                rolled_stim = np.roll(stim_inten[i],stim_shift_x,axis=0)\n",
    "            else:\n",
    "                rolled_stim = np.roll(stim_inten[i],stim_shift_y,axis=0)\n",
    "        stim_inten[i] = rolled_stim\n",
    "        \n",
    "    return stim_inten\n",
    "\n",
    "def saccade_distances(eye_position):\n",
    "    \"\"\"Create a mask for the eye position timeserie that indicate how far was the last saccade.\n",
    "    The eye positions need to be smoothed with smooth_eye_position\"\"\"\n",
    "    \n",
    "    x_pos, y_pos = eye_position[:,0], eye_position[:,1]\n",
    "    saccade_pos = np.where((x_pos[1:] != x_pos[:-1]) & (y_pos[1:] != y_pos[:-1]))[0] + 1\n",
    "    \n",
    "    len_chunks = [saccade_pos[0]]+list(saccade_pos[1:]-saccade_pos[:-1])\n",
    "    len_chunks.append(len(x_pos) - saccade_pos[-1])\n",
    "    \n",
    "    saccade_mask = []\n",
    "    for len_chunk in len_chunks:\n",
    "        saccade_mask.extend(list(range(len_chunk)))\n",
    "    \n",
    "    return np.array(saccade_mask)\n",
    "\n",
    "def smooth_eye_position(eye_position, threshold=2):\n",
    "    x_pos, y_pos = eye_position[:,0], eye_position[:,1]\n",
    "\n",
    "    X = np.stack((x_pos,y_pos, np.linspace(0, len(x_pos)/2, len(x_pos)))).T\n",
    "    clusters = cluster.dbscan(X, eps=threshold, min_samples=3, metric='minkowski', p=2)\n",
    "\n",
    "    move_events = np.where(clusters[1][1:] > clusters[1][:-1])[0] + 1\n",
    "    \n",
    "    len_chunks = [move_events[0]]+list(move_events[1:]-move_events[:-1])\n",
    "    len_chunks.append(len(x_pos) - move_events[-1])\n",
    "\n",
    "    eye_x_positions = np.split(x_pos, move_events)\n",
    "    eye_y_positions = np.split(y_pos, move_events)\n",
    "\n",
    "    mean_x_pos = np.array(list(map(np.mean, eye_x_positions)))\n",
    "    mean_y_pos = np.array(list(map(np.mean, eye_y_positions)))\n",
    "    \n",
    "    x_pos_smooth = np.concatenate([[x_pos]*len_chunk for x_pos,len_chunk in zip(mean_x_pos, len_chunks)])\n",
    "    y_pos_smooth = np.concatenate([[y_pos]*len_chunk for y_pos,len_chunk in zip(mean_y_pos, len_chunks)])\n",
    "    \n",
    "    return np.stack((x_pos_smooth, y_pos_smooth)).T\n",
    "\n",
    "def _eye_to_stim_f(eye_calib, box_width, box_height):\n",
    "    eye_to_stim     = np.linalg.inv(eye_calib)\n",
    "    box_dim         = np.array([1280/(box_width), 720/(box_height)])\n",
    "    return partial(_linear_transform, box_dim=box_dim, transfo_matrix=eye_to_stim)\n",
    "    \n",
    "def _linear_transform(box_dim, transfo_matrix, x_eyeShift, y_eyeShift):\n",
    "    transform_coord = np.dot(transfo_matrix, np.array([x_eyeShift, y_eyeShift]).T)\n",
    "    stim_vec        = np.round(transform_coord * box_dim).astype(int)\n",
    "    return stim_vec[0], -stim_vec[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def process_sta_batch(stim_inten, spike_counts, Hw=30, Fw=2, return_pval=False):\n",
    "    \"\"\"Calculate the STA for a batch of cells.\"\"\"\n",
    "    #Preparing the stimulus\n",
    "    orig_shape = stim_inten.shape\n",
    "    stim_inten = stim_inten_norm(stim_inten)\n",
    "    sum_spikes = np.sum(spike_counts, axis=0)\n",
    "    len_stim = len(stim_inten)\n",
    "    \n",
    "    #We just have to calculate one STA over the whole record\n",
    "    stim_inten   = np.reshape(stim_inten, (len(stim_inten),-1))\n",
    "    stim_inten   = np.transpose(stim_inten)\n",
    "    allCells_sta = staEst_fromBins(stim_inten, spike_counts, Hw, Fw=Fw)\n",
    "\n",
    "    if len(orig_shape)==3:\n",
    "        allCells_sta = allCells_sta.reshape((len(allCells_sta),Hw+Fw, orig_shape[-2], orig_shape[-1]))\n",
    "    elif len(orig_shape)==2:\n",
    "        allCells_sta = allCells_sta.reshape((len(allCells_sta),Hw+Fw,-1))\n",
    "    else:\n",
    "        allCells_sta = allCells_sta.reshape((len(allCells_sta),Hw+Fw))\n",
    "        \n",
    "    if allCells_sta.shape[0]==1: #Only one cell, but we need to keep the axis\n",
    "        allCells_sta = np.squeeze(allCells_sta)\n",
    "        allCells_sta = np.expand_dims(allCells_sta, axis=0)\n",
    "    else:\n",
    "        allCells_sta = np.squeeze(allCells_sta)\n",
    "    \n",
    "    if return_pval:\n",
    "        p_values = np.empty(allCells_sta.shape)\n",
    "    for k, cell_sta in enumerate(allCells_sta): #Easy way to do normalization for each cell that works for all possible shapes\n",
    "        if return_pval:\n",
    "            z_scores    = cell_sta/ np.sqrt(1/sum_spikes[k]) #Standard score is calculated as (x-mean)/std\n",
    "            p_values[k] = sp.stats.norm.sf(abs(z_scores))*2\n",
    "            \n",
    "        allCells_sta[k] = np.nan_to_num(cell_sta/np.max(np.abs(cell_sta)))\n",
    "        \n",
    "    if return_pval:\n",
    "        return allCells_sta, p_values\n",
    "    else:\n",
    "        return allCells_sta\n",
    "    \n",
    "def staEst_fromBins(stim, spike_counts, Hw, Fw=0):\n",
    "    \"\"\"Fw is the amount of frame after a spike that we calculate the average for.\n",
    "        stim must be of shape (x*y,n_frame)\n",
    "        spike_counts must be of shape (n_frame, n_cells)\n",
    "        Return sta of all cells in the shape (n_cells, Hw+Fw, y*x) \"\"\"\n",
    "    spike_counts[:Hw] = 0\n",
    "    \n",
    "    spike_counts = np.nan_to_num(spike_counts / np.sum(spike_counts,axis=0))\n",
    "    spike_counts = spike_counts - np.mean(spike_counts,axis=0)\n",
    "    sta = np.zeros((Hw+Fw, stim.shape[0], spike_counts.shape[-1]))\n",
    "    for i in range(Hw):\n",
    "        sta[(Hw-1-i),:,:] = np.dot(stim, spike_counts)\n",
    "        spike_counts = np.roll(spike_counts, -1, axis=0)\n",
    "    spike_counts = np.roll(spike_counts, Hw, axis=0)\n",
    "    if Fw != 0:\n",
    "        spike_counts[-Fw:] = 0\n",
    "    for i in range(Fw):\n",
    "        spike_counts  = np.roll(spike_counts, 1, axis=0)\n",
    "        sta[Hw+i,:,:] = np.dot(stim, spike_counts)\n",
    "    spike_counts = np.roll(spike_counts, -Fw, axis=0)\n",
    "    return np.transpose(sta, (2,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cross_correlation(spike_counts, tail_len=50):\n",
    "    \"\"\"From `spike_counts` of shape (n_dpoints, n_cell), calculate the cross correlation\n",
    "    of the cells over a window of `tail_len`*2 centered on the middle of the trace.\"\"\"\n",
    "    n_dpoints, n_cell = spike_counts.shape\n",
    "    corr_arr = np.zeros((n_cell,n_cell,tail_len*2+1))\n",
    "    spike_counts = (spike_counts / np.max(spike_counts, axis=0)) #Independant normalization of the cells\n",
    "    spike_counts_edged = np.concatenate((np.zeros((tail_len,n_cell)), \n",
    "                    spike_counts, \n",
    "                    np.zeros((tail_len,n_cell)))) #Creating an array with 0 tails on both side to use the valid mode\n",
    "                                                  # of numpy.correlate\n",
    "    for i in range(n_cell):\n",
    "        for j in range(i, n_cell):\n",
    "            corr_arr[i,j] = np.correlate(spike_counts_edged[:,i],\n",
    "                                         spike_counts[:,j], \n",
    "                                         mode=\"valid\")\n",
    "            corr_arr[j,i] = corr_arr[i,j]\n",
    "    return corr_arr/n_dpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def corrcoef(spike_counts):\n",
    "    return np.corrcoef(spike_counts.T)\n",
    "\n",
    "def flatten_corrcoef(corrcoef_matrix):\n",
    "    shp = corrcoef_matrix.shape\n",
    "    return np.array([corrcoef_matrix[i,j] for i in range(shp[0]) for j in range(i+1, shp[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def stimulus_ensemble(stim_inten, Hw=30, x=0, y=0, w=None, h=None):\n",
    "    stim_inten = stim_inten_norm(stim_inten)\n",
    "    if w is None:\n",
    "        w = stim_inten.shape[2]\n",
    "    if h is None:\n",
    "        h = stim_inten.shape[1]\n",
    "    xmin, xmax = max(0,x-w), min(stim_inten.shape[2], x+w+1)\n",
    "    ymin, ymax = max(0,y-h), min(stim_inten.shape[1], y+h+1)\n",
    "    dtype = stim_inten.dtype\n",
    "    if np.all(np.in1d(stim_inten, [-1,0,1])):\n",
    "        dtype = \"int8\"\n",
    "    stim_ensmbl = np.zeros((len(stim_inten)-Hw, (xmax-xmin)*(ymax-ymin)*Hw), dtype=dtype)\n",
    "    for i in range(Hw, len(stim_inten)):\n",
    "        flat_stim         = np.ndarray.flatten(stim_inten[i-Hw:i,\n",
    "                                                          ymin:ymax,\n",
    "                                                          xmin:xmax])\n",
    "        stim_ensmbl[i-Hw] = flat_stim\n",
    "    return stim_ensmbl\n",
    "\n",
    "def process_nonlinearity(stim_ensemble, spike_bins):\n",
    "    \"\"\"Stimulus must already have been converted to the stim_ensemble, so spike_bins must also\n",
    "    not include the history window a the beggining.\"\"\"\n",
    "    assert len(stim_ensemble)==len(spike_bins)\n",
    "    stim_ensmbl_mean  = np.mean(stim_ensemble,axis=0)#np.mean(spike_triggering_stimuli,axis=0)\n",
    "    spike_ensmbl_mean = np.average(stim_ensemble, axis=0, weights=spike_bins)\n",
    "    middle_vec = np.mean(np.stack((stim_ensmbl_mean, spike_ensmbl_mean)), axis=0)\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    fit = pca.fit(np.stack((stim_ensmbl_mean,\n",
    "                            spike_ensmbl_mean,\n",
    "                            middle_vec)))\n",
    "    stim_ensemble_tranfo = fit.transform(stim_ensemble)\n",
    "    \n",
    "    if np.min(spike_bins)<1:#We have probabilities, not spike counts. Need to make it integers\n",
    "        mask        = np.where(spike_bins > 0)[0]\n",
    "        nonzero_min = np.min(spike_bins[mask])\n",
    "        discretized = spike_bins/nonzero_min\n",
    "        spike_bins  = ((10*discretized)/(np.max(discretized))).astype(int)\n",
    "    spike_ensembl = []\n",
    "    for n_spike, stim_transfo in zip(spike_bins, stim_ensemble_tranfo):\n",
    "        spike_ensembl.extend([stim_transfo]*n_spike)\n",
    "    \n",
    "    xaxis      = np.linspace(np.min(stim_ensemble_tranfo),np.max(stim_ensemble_tranfo),101)\n",
    "    hist_all   = np.histogram(stim_ensemble_tranfo, bins=xaxis)[0]\n",
    "    hist_trigg = np.histogram(spike_ensembl, bins=xaxis)[0]\n",
    "    \n",
    "    nonlin = hist_trigg/hist_all\n",
    "    \n",
    "    nonlin = fill_nan(nonlin)\n",
    "        \n",
    "    return nonlin\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def activity_histogram(spike_counts):\n",
    "    flat_spikes = spike_counts.reshape(-1)\n",
    "    flat_cell = np.array([[i]*spike_counts.shape[0] for i in range(spike_counts.shape[1])]).reshape(-1)\n",
    "    hist = np.histogram2d(flat_spikes, flat_cell, bins=[100,spike_counts.shape[1]])[0] / spike_counts.shape[0]\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cross_distances(masks):\n",
    "    \"\"\"Compute cross distances from the center of mass of a list of mask. masks must be in shape (n_mask, y, x)\"\"\"\n",
    "    center_mass = np.array([ndimage.measurements.center_of_mass(mask) for mask in masks])\n",
    "    cross_distances = np.zeros((len(masks),len(masks)))\n",
    "    for i in range(len(masks)):\n",
    "        for j in range(i,len(masks)):\n",
    "            cross_distances[i,j] = np.linalg.norm(center_mass[i]-center_mass[j])\n",
    "            cross_distances[j,i] = cross_distances[i,j]\n",
    "    return cross_distances\n",
    "\n",
    "def cross_distances_sta(fits, sta_shape, f):\n",
    "    sta_masks = np.array([img_2d_fit(sta_shape, fit, f) for fit in fits])\n",
    "    for i,sta_mask in enumerate(sta_masks):\n",
    "        if abs(np.min(sta_mask)) > np.max(sta_mask):\n",
    "            sta_masks[i] = sta_mask < -.5\n",
    "        else:\n",
    "            sta_masks[i] = sta_mask > .5\n",
    "    return cross_distances(sta_masks)\n",
    "\n",
    "def paired_distances(masks_1, masks_2):\n",
    "    \"\"\"Compute paired distances from the center of mass of a list of mask. masks must be in shape (n_mask, y, x)\"\"\"\n",
    "    center_mass_1 = np.array([ndimage.measurements.center_of_mass(mask) for mask in masks_1])\n",
    "    center_mass_2 = np.array([ndimage.measurements.center_of_mass(mask) for mask in masks_2])\n",
    "    paired_distances = np.zeros(len(masks_1))\n",
    "    for i, (center_1, center_2) in enumerate(zip(masks_1, masks_2)):\n",
    "        paired_distances[i] = np.linalg.norm(center_1-center_2)\n",
    "    return paired_distances\n",
    "\n",
    "def paired_distances_sta(sta_fits_1, sta_fits_2, sta_shape, f):\n",
    "    sta_masks_1 = np.array([img_2d_fit(sta_shape, fit, f) for fit in sta_fits_1])\n",
    "    for i,sta_mask in enumerate(sta_masks_1):\n",
    "        if abs(np.min(sta_mask)) > np.max(sta_mask):\n",
    "            sta_masks_1[i] = sta_mask < -.5\n",
    "        else:\n",
    "            sta_masks_1[i] = sta_mask > .5\n",
    "    sta_masks_2 = np.array([img_2d_fit(sta_shape, fit, f) for fit in sta_fits_2])\n",
    "    for i,sta_mask in enumerate(sta_masks_2):\n",
    "        if abs(np.min(sta_mask)) > np.max(sta_mask):\n",
    "            sta_masks_2[i] = sta_mask < -.5\n",
    "        else:\n",
    "            sta_masks_2[i] = sta_mask > .5\n",
    "    return paired_distances(sta_masks_1, sta_masks_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def direction_selectivity(grouped_spikes_d):\n",
    "    \"\"\"Compute the direction selectivity index of cells in the given dict containing for each condition as\n",
    "    the keys, an array of shape (n_angle, n_repeat, trial_len, n_cell). Such dictionnary can be obtained\n",
    "    by using utils.group_direction_response\n",
    "    Return a dictionnary containing for each condition a tuple:\n",
    "        - spike_sum: a sum of the spikes for each angle for each cell\n",
    "        - dir_pref : an imaginary number of the prefered direction for each cell\n",
    "        - the direction selectivity index of each cell\"\"\"\n",
    "    \n",
    "    res_d = {}\n",
    "    for cond, sp_count in grouped_spikes_d.items():\n",
    "        n_angle = sp_count.shape[0]\n",
    "        mean_n_spike = np.sum(sp_count, axis=(1,2)).T\n",
    "\n",
    "        x         = np.linspace(0, (n_angle-1)/4*np.pi, num=n_angle)\n",
    "\n",
    "        #Direction selectivity\n",
    "        vect_dir  = np.exp(x*1j)#np.array([np.cos(x) + np.sin(x)*1j])\n",
    "        dir_pref  = np.nan_to_num((vect_dir * mean_n_spike).sum(axis=1) / mean_n_spike.sum(axis=1))\n",
    "        ds_idx    = abs(dir_pref)\n",
    "\n",
    "        #Orientation selectivity\n",
    "        vect_ori  = np.exp(x*1j*2)#np.concatenate((vect_dir[:,:n_angle//2], vect_dir[:,:n_angle//2]), axis=1)\n",
    "        ori_pref  = np.nan_to_num((vect_ori * mean_n_spike).sum(axis=1) / mean_n_spike.sum(axis=1))\n",
    "        ori_idx   = abs(ori_pref)\n",
    "\n",
    "        #Generating direction and orientation index from shuffled trials\n",
    "        axtup_l = list(itertools.product(range(sp_count.shape[0]), range(sp_count.shape[1])))\n",
    "        random.seed(1)\n",
    "        n_shuffle = 1000\n",
    "        axtup_l_shuffled = axtup_l.copy()\n",
    "        rand_ori_idx_l = np.empty((n_shuffle, sp_count.shape[3]))\n",
    "        rand_dir_idx_l = np.empty((n_shuffle, sp_count.shape[3]))\n",
    "        for i in range(n_shuffle):\n",
    "            random.shuffle(axtup_l_shuffled)\n",
    "            shuffled_sp_count = np.empty(sp_count.shape)\n",
    "            for axtup, axtup_shuff in zip(axtup_l, axtup_l_shuffled):\n",
    "                shuffled_sp_count[axtup] = sp_count[axtup_shuff]\n",
    "            rand_mean_n_spike      = np.sum(shuffled_sp_count, axis=(1,2)).T\n",
    "            rand_dir_pref     = np.nan_to_num((vect_dir * rand_mean_n_spike).sum(axis=1) / rand_mean_n_spike.sum(axis=1))\n",
    "            rand_dir_idx_l[i] = abs(rand_dir_pref)\n",
    "\n",
    "            rand_ori_pref     = np.nan_to_num((vect_ori * rand_mean_n_spike).sum(axis=1) / rand_mean_n_spike.sum(axis=1))\n",
    "            rand_ori_idx_l[i] = abs(rand_ori_pref)\n",
    "\n",
    "        #Same calculation of pval as in Baden et al 2016\n",
    "        p_val_dir = 1 - (np.sum(rand_dir_idx_l<dir_pref, axis=0)/n_shuffle)\n",
    "        p_val_ori = 1 - (np.sum(rand_ori_idx_l<ori_pref, axis=0)/n_shuffle)\n",
    "\n",
    "        #Finally we have to transform the orientation selectivity vectors to put them back in their\n",
    "        # original orientation, by divinding the phase of the vector by two\n",
    "        tau = np.pi*2\n",
    "        polar_ori_pref = np.array(list((map(polar, ori_pref))))\n",
    "        polar_ori_pref[:,1] = ((polar_ori_pref[:,1]+tau)%tau)/2 #Convert to positive radian angle and divide by two\n",
    "        ori_pref = np.array([rect(pol[0], pol[1]) for pol in polar_ori_pref])\n",
    "\n",
    "        res_d[cond] = (mean_n_spike, dir_pref, ds_idx, ori_pref, ori_idx, p_val_dir, p_val_ori)\n",
    "        \n",
    "    return res_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def peri_saccadic_response(spike_bins, eye_tracking, motion_threshold=5, window=15):    \n",
    "    eye_shifts = np.concatenate(([0],\n",
    "                                 np.linalg.norm(eye_tracking[1:,:2]-eye_tracking[:-1,:2], axis=1)))\n",
    "    \n",
    "    #Because eye tracking is usually upsampled from 15 to 60Hz, it sums the shift, and smooth the peak\n",
    "    # detection\n",
    "    summed_shifts = np.convolve(eye_shifts, [1,1,1,1,1,1,1], mode=\"same\")\n",
    "    \n",
    "    peaks, res = signal.find_peaks(eye_shifts, height=motion_threshold, distance=10)\n",
    "    heights = res[\"peak_heights\"] #Not used for now\n",
    "    \n",
    "    psr = np.zeros((window*2, spike_bins.shape[1]))\n",
    "    for peak in peaks:\n",
    "        if peak<window or (peak+window)>len(spike_bins):\n",
    "            continue #Just ignoring peaks too close to the matrix edges\n",
    "        psr += spike_bins[peak-window:peak+window]\n",
    "    psr /= len(peaks)\n",
    "    return psr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_utils.ipynb.\n",
      "Converted 02_processing.ipynb.\n",
      "Converted 03_modelling.ipynb.\n",
      "Converted 04_plotting.ipynb.\n",
      "Converted 05_database.ipynb.\n",
      "Converted 10_synchro.io.ipynb.\n",
      "Converted 11_synchro.extracting.ipynb.\n",
      "Converted 12_synchro.processing.ipynb.\n",
      "Converted 13_leddome.ipynb.\n",
      "Converted 99_testdata.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
