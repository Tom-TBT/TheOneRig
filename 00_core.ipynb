{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "> Core of this library: DataChunk, RecordMaster, Data_Pipe, and import/export of reM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import h5py\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import namedtuple \n",
    "from typing import Dict, Tuple, Sequence, Union\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "class DataChunk(np.ndarray):\n",
    "    \"\"\"Base brick of data. Derived from np.ndarray\n",
    "    params:\n",
    "        - data: The ndarray with shape (time, ...)\n",
    "        - idx: Index of the start of the DataChunk in the record.\n",
    "        - group: group of the DataChunk in {stim, sync, cell, data}\n",
    "        - fill: Default filling value.\"\"\"\n",
    "    def __new__(cls, data, idx, group, fill=0):\n",
    "        # See https://docs.scipy.org/doc/numpy-1.11.0/user/basics.subclassing.html#basics-subclassing\n",
    "        # for explanation on subclassing numpy arrays\n",
    "        obj = np.asarray(data).view(cls)\n",
    "        obj.idx = idx\n",
    "        obj.group = group\n",
    "        obj.fill = fill\n",
    "        \n",
    "        obj.attrs = {}\n",
    "        \n",
    "        return obj\n",
    "\n",
    "    def __array_finalize__(self, obj):\n",
    "        if obj is None: return\n",
    "        self.idx   = getattr(obj, 'idx', None)\n",
    "        self.group = getattr(obj, 'group', None)\n",
    "        self.fill  = getattr(obj, 'fill', 0)\n",
    "        self.attrs = getattr(obj, 'attrs', {})\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def range(self):\n",
    "        return range(self.idx, self.idx + len(self))\n",
    "    \n",
    "    @property\n",
    "    def slice(self):\n",
    "        return slice(self.idx, self.idx + len(self))\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\"Group: \"+str(self.group)\n",
    "                +\"\\nStarting index: \"+str(self.idx)\n",
    "                +\"\\nFilling value: \"+str(self.fill)\n",
    "                +\"\\n\"+super().__str__())\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"DataChunk(%s,%s,%s,%s)\"%(self.shape, self.idx, self.group, self.fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from theonerig.testdata import *\n",
    "load_vivo_2p(\"./files/vivo_2p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ContiguousRecord():\n",
    "    \"\"\"Representation of a contiguous recording session to store DataChunk\n",
    "    of various sources under a single time reference. DataChunk are stored\n",
    "    under a name in one of the groups \"sync\",\"stim\",\"data\" and \"cell\". \n",
    "    \n",
    "    A name can contain multiple DataChunk if those are not overlapping in time.\n",
    "    \n",
    "    Each ContiguousRecord contains in the group \"sync\" two master DataChunk,\n",
    "    one for signals to be recorded across acquisition device to syncronize them,\n",
    "    one for timepoints of these signals for the main device and are called \n",
    "    respectively \"signals\" and \"main_tp\".\n",
    "    \n",
    "    \"\"\"\n",
    "    MAIN_TP = \"main_tp\"\n",
    "    SIGNALS = \"signals\"\n",
    "    def __init__(self, length:int, signals:DataChunk, main_tp:DataChunk):\n",
    "        \"\"\"Instanciate a ContiguousRecord.\n",
    "        \n",
    "        Parameters:\n",
    "            length (int): Number of bins of this record\n",
    "            signals (DataChunk): Signals for this record\n",
    "            main_tp (DataChunk): Timepoints of the signals for the main device\n",
    "        \"\"\"\n",
    "        self.length = length\n",
    "        self._data_dict = {}  \n",
    "        \n",
    "        self[self.SIGNALS] = signals\n",
    "        self[self.MAIN_TP] = main_tp\n",
    "        \n",
    "        self._slice = slice(0,self.length,1)\n",
    "      \n",
    "    def dataset_intersect(self, existing_datachunk:list, new_datachunk:DataChunk):\n",
    "        range_new = set(new_datachunk.range)\n",
    "        intersect = False\n",
    "        for range_existing in existing_datachunk:\n",
    "            intersect |= len(range_new.intersection(range_existing.range)) > 0\n",
    "        return intersect\n",
    "    \n",
    "    def keys(self):\n",
    "        return self._data_dict.keys()\n",
    "    \n",
    "    def get_slice(self, datachunk_name:str) -> list:\n",
    "        if datachunk_name in self._data_dict.keys():\n",
    "            return [chunk.slice for chunk in self._data_dict[datachunk_name]]\n",
    "        else:\n",
    "            return []\n",
    "        \n",
    "    def set_slice(self, slice_):\n",
    "        \"\"\"Set the slice to restrict the size of the DataChunk returned\"\"\"\n",
    "        if slice_ is None:\n",
    "            self._slice = slice(0,self.length,1)\n",
    "        else:\n",
    "            start, stop, step = slice_.start, slice_.stop, slice_.step\n",
    "            if start is None:\n",
    "                start = 0\n",
    "            if stop is None:\n",
    "                stop = self.length\n",
    "            if step is None:\n",
    "                step = 1\n",
    "            if step!=1:\n",
    "                print(\"Step in slice is currently not supported.\")\n",
    "                self._slice = slice(0,self.length,1)\n",
    "            else:\n",
    "                self._slice = slice(start,stop,step)\n",
    "                    \n",
    "    \n",
    "    def get_names_group(self, group_name:str) -> list:\n",
    "        names = []\n",
    "        for key, dChunk_l in self._data_dict.items():\n",
    "            if dChunk_l[0].group == group_name:\n",
    "                names.append(key)\n",
    "        return names\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __setitem__(self, key, value:DataChunk):\n",
    "        if isinstance(key, str):\n",
    "            if key not in self._data_dict.keys():\n",
    "                self._data_dict[key] = []\n",
    "                \n",
    "            if not self.dataset_intersect(self._data_dict[key], value):\n",
    "                self._data_dict[key].append(value)\n",
    "            else:\n",
    "                raise ValueError(\"Data with the same name already exists and intersect with the one provided\")\n",
    "        else:\n",
    "            raise KeyError(\"Cannot set data with an integer index, it needs a name\")\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, str):\n",
    "            l_datachunk = self._data_dict[key]\n",
    "            fill_value  = l_datachunk[0].fill\n",
    "            shape       = l_datachunk[0].shape\n",
    "            \n",
    "            full_sequence = DataChunk(np.zeros((len(range(*self._slice.indices(self.length))), *shape[1:]), \n",
    "                                               dtype=l_datachunk[0].dtype)+fill_value, \n",
    "                                      self._slice.start if self._slice.start is not None else 0, \n",
    "                                      fill_value)\n",
    "            for datachunk in l_datachunk:\n",
    "                dc_slice = datachunk.slice\n",
    "                if dc_slice.start>=self._slice.stop or dc_slice.stop<=self._slice.start:\n",
    "                    continue\n",
    "                \n",
    "                start = max(dc_slice.start, self._slice.start) #flooring to the maximum of both start\n",
    "                stop  = min(dc_slice.stop, self._slice.stop) # and capping to the min of both end\n",
    "                    \n",
    "                new_dc_slice  = slice(start-datachunk.idx, stop-datachunk.idx)\n",
    "                res_start = self._slice.start if self._slice.start is not None else 0\n",
    "                res_slice = slice(start-res_start, stop-res_start)\n",
    "                full_sequence[res_slice] = datachunk.data[new_dc_slice]\n",
    "                full_sequence.attrs.update(datachunk.attrs)\n",
    "            \n",
    "            return full_sequence\n",
    "                \n",
    "    def __iter__(self):             \n",
    "        groups = {\"sync\":[],\"stim\":[],\"data\":[],\"cell\":[]}\n",
    "        for key, dChunk_l in self._data_dict.items():\n",
    "            groups[dChunk_l[0].group].append((dChunk_l[0].idx, key))\n",
    "\n",
    "        self._iter_order = []\n",
    "        for group_name in [\"sync\",\"stim\",\"data\",\"cell\"]:\n",
    "            sorted_ = sorted(groups[group_name], key=lambda e:(e[0],))\n",
    "            self._iter_order.extend([key for _, key in sorted_])\n",
    "        self._n = 0\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._n < len(self._iter_order):\n",
    "            key = self._iter_order[self._n]\n",
    "            dChunk_l = self._data_dict[key]\n",
    "            self._n += 1\n",
    "            return (key, dChunk_l)\n",
    "        else:\n",
    "            raise StopIteration\n",
    "            \n",
    "    def __delitem__(self, key):\n",
    "        del self._data_dict[key]\n",
    "        \n",
    "    def __str__(self):\n",
    "        res = \"ContiguousRecord:\\n\"\n",
    "        for k,v in self._data_dict.items():\n",
    "            res += k+\" : \"+\" \".join([str(dc.shape) for dc in v]) +\"\\n\"\n",
    "        return res\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self._data_dict.__repr__()\n",
    "    \n",
    "    def __delete__(self, instance):\n",
    "        for k, v in self._data_dict.items():\n",
    "            del v\n",
    "            self._data_dict[k] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RecordMaster(list):\n",
    "    \"\"\"\n",
    "    One Timeserie to rule them all, One Timeserie to find them,\n",
    "    One Timeserie to bring them all and in the darkness bind them.\n",
    "    \n",
    "    The RecordMaster class is the top level object managing all\n",
    "    timeseries. It uses a list of ContiguousRecord to represent\n",
    "    possible discontinuted data records.\n",
    "    \n",
    "    The main aim of the RecordMaster is to store the various data\n",
    "    stream of an experiment under a unique time reference, to ease\n",
    "    the processing of the data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reference_data_list: Sequence[Tuple[DataChunk, DataChunk]], frame_time=1/60, sep_size=1000):\n",
    "        \n",
    "        self.frame_time = frame_time\n",
    "        self.sep_size   = sep_size\n",
    "        self._sequences = []\n",
    "        for ref_timepoints, ref_signals in reference_data_list:\n",
    "            cs = ContiguousRecord(len(ref_timepoints), ref_signals, ref_timepoints)\n",
    "            self._sequences.append(cs)\n",
    "            \n",
    "    def set_datachunk(self, dc:DataChunk, name:str, sequence_idx=0):\n",
    "        \"\"\"Set the given DataChunk dc for the sequence at sequence_idx under name.\"\"\"\n",
    "        self._sequences[sequence_idx][name] = dc\n",
    "        \n",
    "    def append(self, ref_timepoints:DataChunk, ref_signals:DataChunk):\n",
    "        cs = ContiguousRecord(len(ref_timepoints), ref_signals, ref_timepoints)\n",
    "        self._sequences.append(cs)\n",
    "        \n",
    "    def insert(self, idx:int, ref_timepoints:DataChunk, ref_signals:DataChunk):\n",
    "        cs = ContiguousRecord(len(ref_timepoints), ref_signals, ref_timepoints)\n",
    "        self._sequences.insert(idx, cs)\n",
    "        \n",
    "    def keys(self):\n",
    "        keys = []\n",
    "        for seq in self._sequences:\n",
    "            keys.extend(list(seq.keys()))\n",
    "        return set(keys)\n",
    "        \n",
    "    def __setitem__(self, key, value:DataChunk):\n",
    "        \"\"\"Setting an item directly to the record_master place it in the first sequence\"\"\"\n",
    "        if isinstance(key, str):\n",
    "            self._sequences[0][key] = value\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, (int, np.integer)):\n",
    "            return self._sequences[key]\n",
    "        elif isinstance(key, str):\n",
    "            #We want all the data of that name\n",
    "            res = []\n",
    "            for seq in self._sequences:\n",
    "                res.append(seq[key])\n",
    "            return res\n",
    "\n",
    "        raise TypeError(\"Indexing not understood\")\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self._n = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self._n < len(self):\n",
    "            res = self._sequences[self._n]\n",
    "            self._n += 1\n",
    "            return res\n",
    "        else:\n",
    "            raise StopIteration \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self._sequences)\n",
    "        \n",
    "    def plot(self, ax=None):\n",
    "        colors = {\"sync\":\"cornflowerblue\", \"stim\":\"orange\", \"data\":\"yellowgreen\", \"cell\":\"plum\"}\n",
    "        cursor = 0\n",
    "        y_pos_dict = {}\n",
    "        y_count    = 0\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        ax.invert_yaxis()\n",
    "        ax.xaxis.set_visible(False)\n",
    "        for seq in self._sequences:\n",
    "            for y, (name, dChunk_l) in enumerate(seq):\n",
    "                for dChunk in dChunk_l:\n",
    "                    pos = dChunk.idx + cursor \n",
    "                    ax.barh(name, len(dChunk), left=pos, height=0.8, color=colors[dChunk.group], label=dChunk.group)\n",
    "                    x = pos + len(dChunk)/2\n",
    "                    text = \"{0} -> {1} \".format(self.to_time_str(dChunk.idx), self.to_time_str(dChunk.idx+len(dChunk)))\n",
    "                    if name not in y_pos_dict.keys():\n",
    "                        y_pos_dict[name] = y_count\n",
    "                        y_count+=1\n",
    "                    y_pos = y_pos_dict[name]\n",
    "                    ax.text(x, y_pos, text, ha='center', va='center')\n",
    "            cursor += len(seq) + self.sep_size\n",
    "            \n",
    "        legend_elements = [Patch(facecolor=colors[\"sync\"],label='Synchro'),\n",
    "                           Patch(facecolor=colors[\"data\"],label='Data'),\n",
    "                           Patch(facecolor=colors[\"stim\"],label='Stimulus'),\n",
    "                           Patch(facecolor=colors[\"cell\"],label='Cell'),]\n",
    "        ax.legend(handles=legend_elements, ncol=5, bbox_to_anchor=(0, 1), loc='lower left', fontsize='small')\n",
    "        \n",
    "        ax.set_xlim(-100,cursor)\n",
    "            \n",
    "    def to_s(self, n_frame):\n",
    "        return round(self.frame_time*n_frame,2)\n",
    "    \n",
    "    def to_time_str(self, n_frame):\n",
    "        s = int(self.to_s(n_frame))\n",
    "        m, s = s//60, str(s%60)\n",
    "        h, m = str(m//60), str(m%60)\n",
    "        return \"{0}:{1}:{2}\".format('0'*(2-len(h))+h, '0'*(2-len(m))+m, '0'*(2-len(s))+s)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"[\"+\",\\n\".join([repr(seq) for seq in self._sequences])+\"]\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"[\"+\", \".join([repr(seq) for seq in self._sequences])+\"]\"\n",
    "    \n",
    "    def __delete__(self, instance):\n",
    "        for seq in self._sequences:\n",
    "            del seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Data_Pipe():\n",
    "    def __init__(self, record_master:RecordMaster, data_names:Union[str,list], target_names:Union[str,list]=None):\n",
    "        self.record_master = record_master\n",
    "        if isinstance(data_names, str):\n",
    "            data_names = [data_names]\n",
    "        \n",
    "        if isinstance(target_names, str):\n",
    "            target_names = [target_names]\n",
    "            \n",
    "        if target_names is None:\n",
    "            target_names = data_names\n",
    "        elif len(data_names) != len(target_names):\n",
    "            raise Exception(\"data_names and target_names length must match\")\n",
    "        \n",
    "        self.target_names = target_names\n",
    "        self.data_names = data_names\n",
    "        self._masks     = [np.zeros(len(seq), dtype=bool) for seq in record_master]\n",
    "        self._slices    = []\n",
    "        \n",
    "    def plot(self, newfig=False):\n",
    "        factor = -1\n",
    "        if newfig:\n",
    "            plt.figure()\n",
    "            factor = 1\n",
    "        cursor = 0\n",
    "        for i,seq in enumerate(self.record_master):\n",
    "            plt.plot(range(cursor,cursor+len(seq)), self._masks[i]*factor-1)\n",
    "            cursor += len(seq) + self.record_master.sep_size\n",
    "            \n",
    "    def copy(self):\n",
    "        new_pipe =  Data_Pipe(record_master=self.record_master, \n",
    "                         data_names=self.data_names,\n",
    "                         target_names=self.target_names)\n",
    "        new_pipe._masks = np.copy(self._masks)\n",
    "        new_pipe._slices = self._slices.copy()\n",
    "        return new_pipe\n",
    "        \n",
    "    def _get_dchunk_names(self, names):\n",
    "        if isinstance(names, str):\n",
    "            names = [names]\n",
    "        dchunk_name = []\n",
    "        for seq in self.record_master:\n",
    "            for name in names:\n",
    "                if name in [\"sync\", \"cell\", \"data\", \"stim\"]:\n",
    "                    dchunk_name.extend(seq.get_names_group(name))\n",
    "                else:\n",
    "                    dchunk_name.append(name)\n",
    "        return list(set(dchunk_name))\n",
    "    \n",
    "    def _intersect_names(self):        \n",
    "        for i, seq in enumerate(self.record_master):\n",
    "            for name in self.data_names:\n",
    "                if name not in seq.keys():\n",
    "                    self._masks[i] = np.zeros(len(seq), dtype=bool)\n",
    "                    break\n",
    "            \n",
    "    def _update_slices(self):\n",
    "        self._intersect_names() #Always intersect the names we wanna retrieve \n",
    "        self._slices = []\n",
    "        #Iterating the list of mask (one per seq of the record_master)\n",
    "        for j, mask in enumerate(self._masks):\n",
    "            seq = self.record_master[j]\n",
    "            mask = np.concatenate(([0],mask,[0])) #Putting zeros on the side in case the limits would be ones\n",
    "            for start, stop in np.where(mask[1:]-mask[:-1])[0].reshape(-1,2):#Split the mask where 0 separates the 1\n",
    "                self._slices.append((j, slice(start,stop)))\n",
    "        \n",
    "    def __ior__(self, names:Union[str, list]):\n",
    "        return self.__iadd__(names)\n",
    "    def __or__(self, names:Union[str, list]):\n",
    "        return self.copy().__ior__(names)\n",
    "    \n",
    "    def __iand__(self, names:Union[str, list]):\n",
    "        dchunk_name = self._get_dchunk_names(names)\n",
    "        for i, seq in enumerate(self.record_master):\n",
    "            new_mask = np.zeros(len(seq), dtype=bool)\n",
    "            for name in dchunk_name:\n",
    "                for slice_ in seq.get_slice(name):\n",
    "                    new_mask[slice_] = 1\n",
    "            self._masks[i] &= new_mask                \n",
    "        self._update_slices()\n",
    "        return self\n",
    "    def __and__(self, names:Union[str, list]):\n",
    "        return self.copy().__iand__(names)\n",
    "    \n",
    "    def __ixor__(self, names:Union[str, list]):\n",
    "        dchunk_name = self._get_dchunk_names(names)\n",
    "        for i, seq in enumerate(self.record_master):\n",
    "            new_mask = np.zeros(len(seq), dtype=bool)\n",
    "            for name in dchunk_name:\n",
    "                for slice_ in seq.get_slice(name):\n",
    "                    new_mask[slice_] = 1\n",
    "            self._masks[i] ^= new_mask\n",
    "        self._update_slices()\n",
    "        return self\n",
    "    def __xor__(self, names:Union[str, list]):\n",
    "        return self.copy().__ixor__(names)\n",
    "        \n",
    "    def __iadd__(self, names:Union[str, list]):\n",
    "        dchunk_name = self._get_dchunk_names(names)\n",
    "        for i, seq in enumerate(self.record_master):\n",
    "            for name in dchunk_name:\n",
    "                for slice_ in seq.get_slice(name):\n",
    "                    self._masks[i][slice_] = 1\n",
    "        self._update_slices()\n",
    "        return self\n",
    "    def __add__(self, names:Union[str, list]):\n",
    "        return self.copy().__iadd__(names)\n",
    "    \n",
    "    def __isub__(self, names:Union[str, list]):\n",
    "        dchunk_name = self._get_dchunk_names(names)\n",
    "        for i, seq in enumerate(self.record_master):\n",
    "            for name in dchunk_name:\n",
    "                for slice_ in seq.get_slice(name):\n",
    "                    self._masks[i][slice_] = 0\n",
    "        self._update_slices()\n",
    "        return self\n",
    "    def __sub__(self, names:Union[str, list]):\n",
    "        return self.copy().__isub__(names)\n",
    "                \n",
    "    def __iter__(self):\n",
    "        self._n = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self._n < len(self):\n",
    "            res = {}\n",
    "            seq_idx, _slice = self._slices[self._n]\n",
    "            self.record_master[seq_idx].set_slice(_slice)\n",
    "            for i, name in enumerate(self.data_names):\n",
    "                res[self.target_names[i]] = self.record_master[seq_idx][name]\n",
    "            self.record_master[seq_idx].set_slice(None)\n",
    "            self._n += 1\n",
    "            return res\n",
    "        else:\n",
    "            raise StopIteration \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._slices)\n",
    "            \n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, (int, np.integer)):\n",
    "            seq_idx, _slice = self._slices[key]\n",
    "            self.record_master[seq_idx].set_slice(_slice)\n",
    "            res = {}\n",
    "            for i, name in enumerate(self.data_names):\n",
    "                res[self.target_names[i]] = self.record_master[seq_idx][name]\n",
    "            self.record_master[seq_idx].set_slice(None)\n",
    "            return res\n",
    "        elif isinstance(key, slice):\n",
    "            l_res = []\n",
    "            for seq_idx, _slice in self._slices[key]:\n",
    "                res = {}\n",
    "                for i, name in enumerate(self.data_names):\n",
    "                    res[self.target_names[i]] = self.record_master[seq_idx][name][_slice]\n",
    "                l_res.append(res)\n",
    "            return l_res\n",
    "        else:\n",
    "            raise IndexError (\"only integers and slices (`:`) are valid indices\")\n",
    "      \n",
    "    def __str__(self):\n",
    "        return \"(datachunks, targets, slices), \"+self.__repr__()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Pipe(%s)\"%(repr(self.data_names)+\", \"+repr(self.target_names)+\", \"+repr(self._slices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def export_record(_path, record_master):\n",
    "    \"\"\"Export a Record_Master object to an h5 file, readable outside of this library.\"\"\"\n",
    "    print(\"Exporting the record master\")\n",
    "    with h5py.File(_path, mode=\"w\") as h5_f:\n",
    "        h5_f.attrs[\"frame_time\"] = record_master.frame_time\n",
    "        h5_f.attrs[\"sep_size\"]   = record_master.sep_size\n",
    "        for i, contig in enumerate(record_master):\n",
    "            #create contig\n",
    "            print(\"Contiguous sequence\",i)\n",
    "            cntig_ref = h5_f.create_group(str(i))\n",
    "            cntig_ref.attrs[\"length\"] = contig.length\n",
    "            for key, dc_list in contig._data_dict.items():\n",
    "                #create datastream\n",
    "                print(\"...Entering stream\",key)\n",
    "                stream_ref = cntig_ref.create_group(key)\n",
    "                for datachunk in dc_list:\n",
    "                    print(\"......\",str(datachunk.idx)+\"->\"+str(datachunk.idx+len(datachunk)))\n",
    "                    dset = stream_ref.create_dataset(str(datachunk.idx), data=np.array(datachunk), compression=\"gzip\", compression_opts=4)\n",
    "                    for attr_k, attr_v in datachunk.attrs.items():\n",
    "                        dset.attrs[attr_k] = json.dumps(attr_v)\n",
    "                    dset.attrs[\"__fill\"] = datachunk.fill\n",
    "                    dset.attrs[\"__group\"] = datachunk.group\n",
    "    print()\n",
    "                    \n",
    "def import_record(_path):\n",
    "    \"\"\"Import a Record_Master from an h5 file saved by the export_record function of this library.\"\"\"\n",
    "    print(\"Importing the record master\")\n",
    "    with h5py.File(_path, mode=\"r\") as h5_f:\n",
    "        record_master = None\n",
    "        for j, key_contig in enumerate(h5_f.keys()):\n",
    "            ref_contig = h5_f[key_contig]\n",
    "            stream_d = {}\n",
    "            for i, key_dstream in enumerate(ref_contig.keys()): \n",
    "                ref_dstream = ref_contig[key_dstream]\n",
    "                dchunk_l = []\n",
    "                for key_dc in ref_dstream.keys():\n",
    "                    \n",
    "                    data = ref_dstream[key_dc]\n",
    "                    idx  = int(key_dc)\n",
    "                    attrs = {}\n",
    "                    for k,v in data.attrs.items():\n",
    "                        if k not in  [\"__fill\", \"__group\"]:\n",
    "                            attrs[k] = json.loads(v)\n",
    "                        elif k == \"__fill\":\n",
    "                            fill = v\n",
    "                        elif k == \"__group\":\n",
    "                            group = v\n",
    "                    dchunk = DataChunk(data=data[:], idx=idx, group=group)\n",
    "                    dchunk.attrs = attrs\n",
    "                    dchunk_l.append(dchunk)\n",
    "                    \n",
    "                stream_d[key_dstream] = dchunk_l\n",
    "            if record_master is None:\n",
    "                record_master = RecordMaster([(stream_d[\"main_tp\"][0],stream_d[\"signals\"][0])])\n",
    "            else:\n",
    "                record_master.append(stream_d[\"main_tp\"][0],stream_d[\"signals\"][0])\n",
    "            for kstream, vstream in stream_d.items():\n",
    "                for k, dc in enumerate(vstream):\n",
    "                    if kstream in [\"main_tp\", \"signals\"] and k==0:\n",
    "                        continue\n",
    "                    record_master.set_datachunk(dc, name=kstream, sequence_idx=j)\n",
    "    print()\n",
    "    return record_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_utils.ipynb.\n",
      "Converted 02_processing.ipynb.\n",
      "Converted 03_modelling.ipynb.\n",
      "Converted 04_plotting.ipynb.\n",
      "Converted 05_database.ipynb.\n",
      "Converted 10_synchro.io.ipynb.\n",
      "Converted 11_synchro.extracting.ipynb.\n",
      "Converted 12_synchro.processing.ipynb.\n",
      "Converted 13_leddome.ipynb.\n",
      "Converted 99_testdata.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
