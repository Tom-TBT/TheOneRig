{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.test import test_eq\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n",
    "> Useful functions to reshape/arrange/reduce raw data into clean data to add to the record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from typing import Dict, Tuple, Sequence, Union, Callable\n",
    "import scipy.interpolate as interpolate\n",
    "from scipy.ndimage import convolve1d\n",
    "\n",
    "from theonerig.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def extend_sync_timepoints(timepoints:np.ndarray, signals:np.ndarray, \n",
    "                           up_bound, low_bound=0) -> Tuple[DataChunk, DataChunk]:\n",
    "    \"\"\"From `timepoints` and `signals` list, extend it on the left so it includes `low_bound`, and extend it\n",
    "    up to `up_bound`.\n",
    "    The number of frame added to the left can be found in the signal_chunk.idx\n",
    "    \"\"\"\n",
    "    assert len(timepoints) == len(signals)\n",
    "    timepoints = np.array(timepoints)\n",
    "    signals = np.array(signals)\n",
    "    spb = np.mean(timepoints[1:]-timepoints[:-1]) #spf: sample_per_bin\n",
    "        \n",
    "    #Left and right side are just prolongation of the sample_times up \n",
    "    # from (0-sample_per_fr) to (len+sample_per_fr) so it covers all timepoints\n",
    "    left_side  = np.arange(timepoints[0]-spb , low_bound - spb, -spb)[::-1].astype(int)\n",
    "    right_side = np.arange(timepoints[-1]+spb,  up_bound + spb,  spb).astype(int)\n",
    "\n",
    "    new_timepoints = np.concatenate((left_side, \n",
    "                                     timepoints, \n",
    "                                     right_side))\n",
    "    \n",
    "    timepoint_chunk = DataChunk(data=new_timepoints, idx=0, group=\"sync\")\n",
    "    signal_chunk    = DataChunk(data=signals, idx=len(left_side), group=\"sync\")\n",
    "    return (timepoint_chunk, signal_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It extends the timepoints by finding the typical distance between timepoints so it includes both left and right limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def align_sync_timepoints(timepoints:DataChunk, signals:DataChunk,\n",
    "                          ref_timepoints:DataChunk, ref_signals:DataChunk) -> Tuple[DataChunk, DataChunk, DataChunk]:\n",
    "    \"\"\"Align the `signals` of a `timepoints` timeserie to a reference `ref_timepoints` with the corresponding\n",
    "    `ref_signals`. `ref_timepoints` and `ref_signals` are potentially extended and returned\n",
    "    Returns a DataChunk of the aligned timepoints\"\"\"    \n",
    "    shift_left = ((np.where(ref_signals)[0][0] + ref_signals.idx) \n",
    "                  - (np.where(signals)[0][0] + signals.idx))\n",
    "    shift_right   = len(ref_timepoints) - (len(timepoints) + shift_left) \n",
    "\n",
    "    spb     = np.mean(timepoints[1:]-timepoints[:-1]) #spf: sample_per_bin\n",
    "    spb_ref = np.mean(ref_timepoints[1:]-ref_timepoints[:-1]) #spf: sample_per_bin\n",
    "    \n",
    "    left_timepoints      = np.zeros(0)\n",
    "    left_timepoints_ref  = np.zeros(0)\n",
    "    right_timepoints     = np.zeros(0)\n",
    "    right_timepoints_ref = np.zeros(0)\n",
    "    \n",
    "    if shift_left > 0: #the ref started before, need to extend the other\n",
    "        init  = timepoints[0]-spb\n",
    "        left_timepoints = np.arange(init , \n",
    "                                    init-(spb*shift_left+1), \n",
    "                                    -spb)[:shift_left][::-1].astype(int)\n",
    "    else:\n",
    "        shift_left = abs(shift_left)\n",
    "        init  = ref_timepoints[0]-spb_ref\n",
    "        left_timepoints_ref = np.arange(init , \n",
    "                                        init-(spb_ref*shift_left+1), \n",
    "                                        -spb_ref)[:shift_left][::-1].astype(int)\n",
    "        #We also need to shift the index of the ref signals since we increased the size of the ref_timepoints\n",
    "        ref_signals.idx = ref_signals.idx + len(left_timepoints_ref)\n",
    "        \n",
    "    if shift_right > 0: #the ref ended after, need to extend the other\n",
    "        init  = timepoints[-1]+spb\n",
    "        right_timepoints = np.arange(init , \n",
    "                                    init+(spb*shift_right+1), \n",
    "                                    spb)[:shift_right].astype(int)\n",
    "    else:\n",
    "        shift_right = abs(shift_right)\n",
    "        init  = ref_timepoints[-1]+spb_ref\n",
    "        right_timepoints_ref = np.arange(init , \n",
    "                                    init+(spb_ref*shift_right+1), \n",
    "                                    spb_ref)[:shift_right].astype(int)\n",
    "        \n",
    "    timepoint    = DataChunk(data=np.concatenate((left_timepoints, \n",
    "                                     timepoints, \n",
    "                                     right_timepoints)), idx=0, group=\"sync\")\n",
    "    \n",
    "    timepoint_ref = DataChunk(data=np.concatenate((left_timepoints_ref, \n",
    "                                     ref_timepoints, \n",
    "                                     right_timepoints_ref)), idx=0, group=\"sync\")\n",
    "    \n",
    "    return (timepoint, timepoint_ref, ref_signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def resample_to_timepoints(timepoints:np.ndarray, data:np.ndarray, \n",
    "                             ref_timepoints:DataChunk, group=\"data\") -> DataChunk:\n",
    "    \"\"\"Resample the `data` at the `timepoints` to an array at the timepoints of `ref_timepoints`.\n",
    "    Return a DataChunck of the resampled data belonging to `group`.\"\"\"\n",
    "    \n",
    "    assert len(timepoints) == len(data)\n",
    "    timepoints = np.array(timepoints)\n",
    "    data = np.array(data)\n",
    "    \n",
    "    start_idx = np.argmax(ref_timepoints >= timepoints[0])\n",
    "    stop_idx  = np.argmax(ref_timepoints >= timepoints[-1])\n",
    "    if stop_idx == 0:\n",
    "        stop_idx = len(ref_timepoints)\n",
    "    \n",
    "    if len(ref_timepoints[start_idx:stop_idx]) < len(timepoints): #Downsampling\n",
    "        distance = (np.argmax(timepoints>ref_timepoints[start_idx+1]) \n",
    "                - np.argmax(timepoints>ref_timepoints[start_idx]))\n",
    "    \n",
    "        kernel = np.ones(distance)/distance\n",
    "        data = convolve1d(data, kernel, axis=0) #Smooting to avoid weird sampling\n",
    "\n",
    "    new_data = interpolate.interp1d(timepoints, data, axis=0)(ref_timepoints[start_idx:stop_idx])\n",
    "\n",
    "    idx = ref_timepoints.idx + start_idx\n",
    "    return DataChunk(data=new_data, idx = idx, group=group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def flip_stimulus(stim_inten, ud_inv, lr_inv):\n",
    "    if lr_inv:\n",
    "        stim_inten = np.flip(stim_inten, axis=3) # Axis 0:t 1:color 2:y 3:x\n",
    "    if not ud_inv: \n",
    "        #Numpy and QDSpy orientation are different. \n",
    "        #This way reorientate the stimulus approriatly for display with matplotlib and potential\n",
    "        #eye tracking corrections\n",
    "        stim_inten = np.flip(stim_inten, axis=2)\n",
    "    return stim_inten\n",
    "\n",
    "def flip_gratings(stim_shader, ud_inv, lr_inv):\n",
    "    mask_epochs = ~np.all(stim_shader==0,axis=1)\n",
    "    if lr_inv:\n",
    "        stim_shader[mask_epochs,1] = (360 + (180 - stim_shader[mask_epochs,1])) % 360 \n",
    "    if ud_inv:\n",
    "        stim_shader[mask_epochs,1] = (360 - stim_shader[mask_epochs,1]) % 360\n",
    "    return stim_shader\n",
    "\n",
    "def stim_to_dataChunk(stim_inten, stim_start_idx, reference:DataChunk) -> DataChunk:\n",
    "    \"\"\"Factory function for DataChunk of a stimulus\"\"\"\n",
    "    return DataChunk(data=np.squeeze(stim_inten), idx = (stim_start_idx + reference.idx), group=\"stim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def spike_to_dataChunk(spike_timepoints, ref_timepoints:DataChunk) -> DataChunk:\n",
    "    \"\"\"`spike_timepoints` must be a dictionnary of cell spike_timepoints list. This function then\n",
    "    bins the \"\"\"\n",
    "    type_cast = type(list(spike_timepoints.keys())[0])\n",
    "    cell_keys = sorted(map(int, \n",
    "                                    spike_timepoints.keys()))\n",
    "    cell_map = dict([ (cell_key, i) for i, cell_key in enumerate(cell_keys) ])\n",
    "    spike_bins = np.zeros((ref_timepoints.shape[0], len(cell_keys)))\n",
    "    bins = np.concatenate((ref_timepoints[:], [(ref_timepoints[-1]*2)-ref_timepoints[-2]]))\n",
    "\n",
    "    for i, cell in enumerate(cell_keys):\n",
    "        spike_bins[:, i] = np.histogram(spike_timepoints[type_cast(cell)], bins)[0]\n",
    "        \n",
    "    datachunk = DataChunk(data=spike_bins, idx = ref_timepoints.idx, group=\"cell\")\n",
    "    datachunk.attrs[\"cell_map\"] = cell_map\n",
    "    return datachunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parse_stim_args(stim_name, stim_ref):\n",
    "    \"\"\"Function really specific to Asari Lab stimuli. Stimuli were stored as h5 files. This function parse\n",
    "    the attributes of the stimuli that were stored in the h5 references of the stimuli.\"\"\"\n",
    "    args = {}\n",
    "    if stim_name in [\"chirp_am\", \"chirp_fm\", \"chirp_co\"]:\n",
    "        #+ add on off timings at the beginning?\n",
    "        args[\"n_repeat\"] = int(stim_ref.attrs[\"n_repeat\"])\n",
    "    if stim_name in [\"chirp_fm\"]:\n",
    "        args[\"max_freq\"] = int(stim_ref.attrs[\"max_frequency\"])\n",
    "    if stim_name in [\"moving_gratings\"]:\n",
    "        #+ Convert to degree units\n",
    "        args[\"n_fr_stim\"] = int(stim_ref.attrs[\"n_frame_on\"])#.keys()\n",
    "        args[\"n_fr_interstim\"] = int(stim_ref.attrs[\"n_frame_off\"])\n",
    "        args[\"n_repeat\"] = int(stim_ref.attrs[\"n_repeat\"])\n",
    "        args[\"n_angle\"] = int(stim_ref.attrs[\"n_angle\"])\n",
    "        args[\"sp_freqs\"] = list(map(int,stim_ref.attrs[\"spatial_frequencies\"][1:-1].split(\",\")))\n",
    "        args[\"speeds\"] = list(map(int,stim_ref.attrs[\"speeds\"][1:-1].split(\",\")))\n",
    "    if stim_name in [\"flickering_bars\", \"checkerboard\", \"flickering_bars_pr\"]:\n",
    "        #Get the size of the sides in angle\n",
    "        pass\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def peak_sta_frame(sta):\n",
    "    abs_sta   = np.abs(sta)\n",
    "    idx_frame = np.unravel_index(abs_sta.argmax(), sta.shape)[0]\n",
    "    return sta[idx_frame]\n",
    "\n",
    "def stim_inten_norm(stim_inten):\n",
    "    stim_inten = stim_inten.astype(float)\n",
    "    stim_inten -= np.min(stim_inten)\n",
    "    stim_inten -= np.max(stim_inten)/2\n",
    "    stim_inten /= np.max(np.abs(stim_inten))\n",
    "    return np.round(stim_inten, 2)\n",
    "#     stim_inten[stim_inten==255] = 1\n",
    "#     stim_inten[stim_inten==0]   = -1\n",
    "#     stim_inten[(stim_inten==127) | (stim_inten==128)] = 0 #In case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def twoP_dataChunks(ref_timepoints:DataChunk, frame_timepoints, len_epochs, C_matrix, S_matrix):\n",
    "    C_datachunk_l = []\n",
    "    S_datachunk_l = []\n",
    "    cursor = 0\n",
    "    for i, len_epoch in enumerate(len_epochs):\n",
    "        start_idx = np.argmax(ref_timepoints>frame_timepoints[i][0])\n",
    "        stop_idx  = np.argmax(ref_timepoints>frame_timepoints[i][len_epoch-1])\n",
    "        sub_C, sub_S = C_matrix.T[cursor:cursor+len_epoch], S_matrix.T[cursor:cursor+len_epoch]\n",
    "        cursor += len_epoch\n",
    "        f = interpolate.interp1d(range(len_epoch), sub_C, axis=0)\n",
    "        C_datachunk_l.append(DataChunk(data=f(np.linspace(0,len_epoch-1,stop_idx-start_idx)), \n",
    "                                       idx=start_idx, \n",
    "                                       group=\"cell\"))\n",
    "        f = interpolate.interp1d(range(len_epoch), sub_S, axis=0)\n",
    "        S_datachunk_l.append(DataChunk(data=f(np.linspace(0,len_epoch-1,stop_idx-start_idx)), \n",
    "                                       idx=start_idx, \n",
    "                                       group=\"cell\"))\n",
    "    return (C_datachunk_l, S_datachunk_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def img_2d_fit(shape, param_d, f):\n",
    "    y_, x_ = shape\n",
    "    xy = np.meshgrid(range(x_), range(y_))\n",
    "    return f(xy, **param_d).reshape(y_,x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fill_nan(A):\n",
    "    '''\n",
    "    interpolate to fill nan values. BRYAN WOODS@StackOverflow\n",
    "    '''\n",
    "    inds = np.arange(A.shape[0])\n",
    "    good = np.where(np.isfinite(A))\n",
    "    f = interpolate.interp1d(inds[good], A[good],bounds_error=False)\n",
    "    B = np.where(np.isfinite(A),A,f(inds))\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def group_direction_response(stim_prop, spike_counts, n_repeat, n_cond=32):\n",
    "    \"\"\"Group the record according to conditions.\"\"\"\n",
    "    \n",
    "    n_cell = spike_counts.shape[-1]\n",
    "    condition_repeat = stim_prop.reshape(n_repeat*n_cond,-1,3)[:,1,:]\n",
    "    spike_resh       = spike_counts.reshape(n_repeat*n_cond,-1,n_cell)\n",
    "    angles = np.unique(condition_repeat[:,1])\n",
    "\n",
    "    data_dict = {}\n",
    "    for cond in np.unique(condition_repeat, axis=0):\n",
    "        spat_freq, angle, speed = tuple(cond)\n",
    "        idx_cond = np.argwhere(np.all(condition_repeat==cond, axis=1))[:,0]\n",
    "\n",
    "        cond_key = str(spat_freq)+\"@\"+str(round(speed,2))\n",
    "        if cond_key not in data_dict.keys():\n",
    "            data_dict[cond_key] = np.empty((8, len(idx_cond), *spike_resh[0].shape))\n",
    "\n",
    "        idx_angle = np.where(angle==angles)[0][0]\n",
    "        data_dict[cond_key][idx_angle] = np.array([spike_resh[idx] for idx in idx_cond])\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def group_chirp_bumps(stim_inten, spike_counts, n_repeat):\n",
    "    repeat = stim_inten.reshape(n_repeat,-1)[0]\n",
    "    spike_counts = spike_counts.reshape(n_repeat,-1,spike_counts.shape[-1])\n",
    "    epoch_l = [0]\n",
    "    end_l = [len(repeat)]\n",
    "    i = 1\n",
    "    curr = repeat[0]\n",
    "\n",
    "    while True:\n",
    "        while repeat[i]==curr:\n",
    "            i+=1\n",
    "        epoch_l.append(i)\n",
    "        curr = repeat[i]\n",
    "        if curr==repeat[i+1]:\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    i = len(repeat)-2\n",
    "    curr = repeat[-1]\n",
    "\n",
    "    while True:\n",
    "        while repeat[i]==curr:\n",
    "            i-=1\n",
    "        end_l.insert(0,i)\n",
    "        curr = repeat[i]\n",
    "        if curr==repeat[i-1]:\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    slices = [slice(epoch_l[i-1],epoch_l[i]) for i in range(1,len(epoch_l))]\n",
    "    slices.extend([slice(end_l[i-1],end_l[i]) for i in range(1,len(end_l))])\n",
    "\n",
    "    res_d = {}\n",
    "    for slc in slices:\n",
    "        key = str(stim_inten[slc.start])+\"@\"+str(slc.start)\n",
    "        res_d[key] = spike_counts[:,slc]\n",
    "\n",
    "    return res_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def limited_stim_ensemble(stim_inten, cell_sta, Hw=16, window=4):\n",
    "    y,x = np.argwhere(np.abs(cell_sta)==1)[0][1:]\n",
    "    x_low, x_high = max(0, x-window), min(cell_sta.shape[2]-1, x+window)\n",
    "    y_low, y_high = max(0, y-window), min(cell_sta.shape[1]-1, y+window)\n",
    "    y, x = np.meshgrid(np.linspace(y_low,y_high,window*2+1, dtype=int), \n",
    "                       np.linspace(x_low,x_high,window*2+1, dtype=int))\n",
    "    limited_stim = stim_inten[:,y, x]\n",
    "    \n",
    "    stim_ensemble = np.zeros((len(limited_stim)-Hw, limited_stim.shape[-2]*limited_stim.shape[-1]*Hw))\n",
    "    for i in range(Hw, len(limited_stim)):\n",
    "        flat_stim = np.ndarray.flatten(limited_stim[i-Hw:i]) #,5:11,22:28\n",
    "        stim_ensemble[i-Hw] = flat_stim\n",
    "    return stim_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def buszaki_shank_channels(channel_positions):\n",
    "    shank_1_mask = channel_positions[:,0]<180\n",
    "    shank_1_idx  = np.argwhere(shank_1_mask)[:,0]\n",
    "    shank_2_mask = (channel_positions[:,0]<380) & np.invert(shank_1_mask)\n",
    "    shank_2_idx  = np.argwhere(shank_2_mask)[:,0]\n",
    "    shank_4_mask = channel_positions[:,0]>580\n",
    "    shank_4_idx  = np.argwhere(shank_4_mask)[:,0]\n",
    "    shank_3_mask = (channel_positions[:,0]>380) & np.invert(shank_4_mask)\n",
    "    shank_3_idx  = np.argwhere(shank_3_mask)[:,0]\n",
    "\n",
    "    shanks_idx = np.zeros((4,8), dtype=int) - 1 #Initialize with -1 in case of channel missing\n",
    "    shanks_idx[0,:len(shank_1_idx)] = shank_1_idx\n",
    "    shanks_idx[1,:len(shank_2_idx)] = shank_2_idx\n",
    "    shanks_idx[2,:len(shank_3_idx)] = shank_3_idx\n",
    "    shanks_idx[3,:len(shank_4_idx)] = shank_4_idx\n",
    "    return shanks_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def phy_results_dict(phy_dir):\n",
    "    res_dict = {}\n",
    "    res_dict[\"amplitudes\"] = np.load(phy_dir+\"/amplitudes.npy\")\n",
    "    res_dict[\"channel_map\"] = np.load(phy_dir+\"/channel_map.npy\")\n",
    "    res_dict[\"channel_positions\"] = np.load(phy_dir+\"/channel_positions.npy\")\n",
    "    res_dict[\"spike_clusters\"] = np.load(phy_dir+\"/spike_clusters.npy\")\n",
    "    res_dict[\"spike_templates\"] = np.load(phy_dir+\"/spike_templates.npy\")\n",
    "    res_dict[\"spike_times\"] = np.load(phy_dir+\"/spike_times.npy\")\n",
    "    res_dict[\"templates\"] = np.load(phy_dir+\"/templates.npy\")\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def format_pval(pval, significant_figures=2):\n",
    "    return '{:g}'.format(float('{:.{p}g}'.format(pval, p=significant_figures)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_calcium_stack_lenghts(folder):\n",
    "    record_lenghts = []\n",
    "    pattern_nFrame = r\".*number=(\\d*) .*\"\n",
    "    for fn in glob.glob(folder+\"/*.txt\"):\n",
    "        with open(fn) as f:\n",
    "            line = f.readline()\n",
    "            record_lenghts.append(int(re.findall(pattern_nFrame, line)[0]))\n",
    "    return record_lenghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_chirp_mean_corrected(stim_inten, spike_counts):\n",
    "    \"\"\"Correct the stimulus shifts before averaging the spikes\"\"\"\n",
    "    def count_repl_in_range(fr_replaced, _range):\n",
    "        return sum([repl[0] in _range for repl in fr_replaced])\n",
    "    \n",
    "    conv_res  = np.convolve(stim_inten[360:600].astype(float), stim_inten.astype(float), mode=\"full\")\n",
    "    n_repeats = np.sum(conv_res.max()==conv_res)\n",
    "    \n",
    "    signal_shifts     = stim_inten.attrs[\"signal_shifts\"]\n",
    "    frame_replacement = stim_inten.attrs[\"frame_replacement\"]\n",
    "    \n",
    "    spike_count_corr = spike_counts.copy()\n",
    "    shift_cursor = 0\n",
    "    prev_del = np.zeros((1, spike_counts.shape[1]))\n",
    "    for shift, direction in signal_shifts:\n",
    "        if direction==\"ins\":\n",
    "            spike_count_corr[shift+1:] = spike_count_corr[shift:-1]\n",
    "            prev_del = spike_count_corr[-1:]\n",
    "        else:\n",
    "            spike_count_corr[shift-1:-1] = spike_count_corr[shift:]\n",
    "            spike_count_corr[-1:] = prev_del\n",
    "            \n",
    "    len_epoch = len(stim_inten)//n_repeats\n",
    "    good_spike_counts = []\n",
    "    for i in range(n_repeats):\n",
    "        if count_repl_in_range(frame_replacement, range(len_epoch*i, len_epoch*(i+1)))>20:\n",
    "            continue\n",
    "        good_spike_counts.append(spike_count_corr[len_epoch*i:len_epoch*(i+1)])\n",
    "    return np.mean(good_spike_counts, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def stim_recap_df(reM):\n",
    "    def parse_stim(stim_dc):\n",
    "        param_d = {}\n",
    "        param_d[\"hash\"]        = stim_dc.attrs[\"md5\"][:10] #the first 10 letters are more than enough\n",
    "        param_d[\"n frames\"]    = len(stim_dc)\n",
    "        param_d[\"stimulus\"]    = stim_dc.attrs[\"name\"]\n",
    "\n",
    "        if stim_dc.attrs[\"name\"] in [\"checkerboard\", \"fullfield_flicker\", \"flickering_bars\", \"flickering_bars_pr\"]:\n",
    "            param_d[\"frequency\"] = stim_dc.attrs[\"refresh_rate\"]\n",
    "        elif stim_dc.attrs[\"name\"] in [\"chirp_am\",\"chirp_fm\",\"chirp_freq_epoch\", \"chirp_co\"]:\n",
    "            param_d[\"n ON\"]      = int(stim_dc.attrs[\"tSteadyON_s\"]*60)\n",
    "            param_d[\"n OFF\"]     = int(stim_dc.attrs[\"tSteadyOFF_s\"]*60)\n",
    "            param_d[\"n repeats\"] = int(stim_dc.attrs[\"n_repeat\"])\n",
    "            if stim_dc.attrs[\"name\"] in [\"chirp_am\",\"chirp_co\"]:\n",
    "                param_d[\"frequency\"] = stim_dc.attrs[\"contrast_frequency\"]\n",
    "            elif stim_dc.attrs[\"name\"]==\"chirp_fm\":\n",
    "                param_d[\"frequency\"] = stim_dc.attrs[\"max_frequency\"]\n",
    "            elif stim_dc.attrs[\"name\"]==\"chirp_freq_epoch\":\n",
    "                param_d[\"frequency\"] = str([round(60/nfr,2) for nfr in dc.attrs[\"n_frame_cycle\"]])\n",
    "        elif stim_dc.attrs[\"name\"] in [\"fullfield_color_mix\"]:\n",
    "            param_d[\"n ON\"]      = int(stim_dc.attrs[\"n_frame_on\"])\n",
    "            param_d[\"n OFF\"]     = int(stim_dc.attrs[\"n_frame_off\"])\n",
    "            param_d[\"n repeats\"] = int(stim_dc.attrs[\"n_repeat\"])\n",
    "        elif stim_dc.attrs[\"name\"]==\"moving_gratings\":\n",
    "            param_d[\"n repeats\"]           = stim_dc.attrs[\"n_repeat\"]\n",
    "            param_d[\"n ON\"]                = stim_dc.attrs[\"n_frame_on\"]\n",
    "            param_d[\"n OFF\"]               = stim_dc.attrs[\"n_frame_off\"]\n",
    "            param_d[\"speeds\"]              = stim_dc.attrs[\"speeds\"]\n",
    "            param_d[\"spatial frequencies\"] = stim_dc.attrs[\"spatial_frequencies\"]\n",
    "            \n",
    "        if \"frame_replacement\" in stim_dc.attrs:\n",
    "            param_d[\"total drop\"] = len(stim_dc.attrs[\"frame_replacement\"])\n",
    "        if \"signal_shifts\" in stim_dc.attrs:\n",
    "            shift = 0\n",
    "            for _, which_shift in stim_dc.attrs[\"signal_shifts\"]:\n",
    "                if which_shift==\"ins\":\n",
    "                    shift += 1\n",
    "                elif which_shift==\"del\":\n",
    "                    shift -= 1\n",
    "            param_d[\"total shift\"] = shift\n",
    "\n",
    "        return param_d\n",
    "\n",
    "    df = pd.DataFrame(columns=[\"stimulus\", \"hash\", \"n frames\", \"n repeats\",\n",
    "                               \"frequency\", \"n ON\", \"n OFF\", \"speeds\", \"spatial frequencies\",\n",
    "                              \"total shift\", \"total drop\"])\n",
    "    cursor = 0\n",
    "    for k, dc_l in reM[0]:\n",
    "        dc = dc_l[0]\n",
    "        if dc.group == \"stim\":\n",
    "            serie = pd.Series(data=parse_stim(dc), name=cursor)\n",
    "            df = df.append(serie, ignore_index=False)\n",
    "            cursor+=1\n",
    "\n",
    "    df = df.fillna(\"\")\n",
    "    return df\n",
    "\n",
    "def stim_recap_df_old(h5_stim_group):\n",
    "    df = pd.DataFrame(index=[\"checkerboard\", \"fullfield_flicker\", \"flickering_bars\", \n",
    "                             \"chirp_am\", \"chirp_fm\", \"moving_gratings\"], \n",
    "                     columns=[\"stimulus\", \"hash\", \"n frames\", \"n repeats\", \n",
    "                              \"frequency\", \"n ON\", \"n OFF\", \"speeds\", \"spatial frequencies\"])\n",
    "    df.loc[\"checkerboard\"][\"stimulus\"]     = \"checkerboard\"\n",
    "    df.loc[\"fullfield_flicker\"][\"stimulus\"]= \"fullfield_flicker\"\n",
    "    df.loc[\"flickering_bars\"][\"stimulus\"]  = \"flickering_bars\"\n",
    "    df.loc[\"chirp_am\"][\"stimulus\"]         = \"chirp_am\"\n",
    "    df.loc[\"chirp_fm\"][\"stimulus\"]         = \"chirp_fm\"\n",
    "    df.loc[\"moving_gratings\"][\"stimulus\"]  = \"moving_gratings\"\n",
    "    for stim_key in h5_stim_group.keys():\n",
    "        extract_old_stimulus_metadata(h5_stim_group[stim_key], df)\n",
    "        \n",
    "    df = df.fillna('')\n",
    "    return df\n",
    "\n",
    "    def extract_old_stimulus_metadata(h5_group, df):\n",
    "        stim_name = h5_group.attrs[\"name\"]\n",
    "        nhash_letters = 10\n",
    "        if stim_name==\"checkerboard\":\n",
    "            df.loc[\"checkerboard\"][\"hash\"]      = h5_group.attrs[\"hash\"][:nhash_letters]\n",
    "            df.loc[\"checkerboard\"][\"n frames\"]    = len(h5_group[\"intensity\"])\n",
    "            df.loc[\"checkerboard\"][\"frequency\"]      = h5_group.attrs[\"refresh_rate\"]\n",
    "            df.loc[\"checkerboard\"][\"stimulus\"]  = stim_name\n",
    "\n",
    "        elif stim_name==\"fullfield_flicker\":    \n",
    "            df.loc[\"fullfield_flicker\"][\"hash\"]   = h5_group.attrs[\"hash\"][:nhash_letters]\n",
    "            df.loc[\"fullfield_flicker\"][\"n frames\"] =len(h5_group[\"intensity\"])\n",
    "            df.loc[\"fullfield_flicker\"][\"frequency\"]   = h5_group.attrs[\"refresh_rate\"]\n",
    "            df.loc[\"fullfield_flicker\"][\"stimulus\"]  = stim_name\n",
    "\n",
    "        elif stim_name==\"flickering_bars\" or stim_name==\"flickering_bars_pr\":    \n",
    "            df.loc[\"flickering_bars\"][\"hash\"]   = h5_group.attrs[\"hash\"][:nhash_letters]\n",
    "            df.loc[\"flickering_bars\"][\"n frames\"] = len(h5_group[\"intensity\"])\n",
    "            df.loc[\"flickering_bars\"][\"frequency\"]   = h5_group.attrs[\"refresh_rate\"]\n",
    "            df.loc[\"flickering_bars\"][\"stimulus\"]  = stim_name\n",
    "\n",
    "        elif stim_name==\"chirp_am\":    \n",
    "            df.loc[\"chirp_am\"][\"hash\"]    = h5_group.attrs[\"hash\"][:nhash_letters]\n",
    "            df.loc[\"chirp_am\"][\"n frames\"]  = len(h5_group[\"intensity\"])\n",
    "            df.loc[\"chirp_am\"][\"n repeats\"] = h5_group.attrs[\"n_repeat\"]\n",
    "            df.loc[\"chirp_am\"][\"frequency\"]    = h5_group.attrs[\"contrast_frequency\"]\n",
    "            df.loc[\"chirp_am\"][\"n ON\"]     = int(float(h5_group.attrs[\"tSteadyON_s\"])*60)\n",
    "            df.loc[\"chirp_am\"][\"n OFF\"]    = int(float(h5_group.attrs[\"tSteadyOFF_s\"])*60)\n",
    "            df.loc[\"chirp_am\"][\"stimulus\"]  = stim_name\n",
    "\n",
    "        elif stim_name==\"chirp_freq_epoch\":\n",
    "            df.loc[\"chirp_fm\"][\"hash\"]    = h5_group.attrs[\"hash\"][:nhash_letters]\n",
    "            df.loc[\"chirp_fm\"][\"n frames\"]  = len(h5_group[\"intensity\"])\n",
    "            df.loc[\"chirp_fm\"][\"n repeats\"] = h5_group.attrs[\"n_repeat\"]\n",
    "            df.loc[\"chirp_fm\"][\"n ON\"]     = int(float(h5_group.attrs[\"tSteadyON_s\"])*60)\n",
    "            df.loc[\"chirp_fm\"][\"n OFF\"]    = int(float(h5_group.attrs[\"tSteadyOFF_s\"])*60)\n",
    "            df.loc[\"chirp_fm\"][\"stimulus\"]  = stim_name\n",
    "\n",
    "        elif stim_name==\"moving_gratings\":\n",
    "            df.loc[\"moving_gratings\"][\"hash\"]                = h5_group.attrs[\"hash\"][:nhash_letters]\n",
    "            df.loc[\"moving_gratings\"][\"n frames\"]              = len(h5_group[\"intensity\"])\n",
    "            df.loc[\"moving_gratings\"][\"n repeats\"]             = h5_group.attrs[\"n_repeat\"]\n",
    "            df.loc[\"moving_gratings\"][\"n ON\"]                 = h5_group.attrs[\"n_frame_on\"]\n",
    "            df.loc[\"moving_gratings\"][\"n OFF\"]                = h5_group.attrs[\"n_frame_off\"]\n",
    "            df.loc[\"moving_gratings\"][\"speeds\"]              = h5_group.attrs[\"speeds\"]\n",
    "            df.loc[\"moving_gratings\"][\"spatial frequencies\"] = h5_group.attrs[\"spatial_frequencies\"]\n",
    "            df.loc[\"moving_gratings\"][\"stimulus\"]  = stim_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_utils.ipynb.\n",
      "Converted 02_processing.ipynb.\n",
      "Converted 03_modelling.ipynb.\n",
      "Converted 04_plotting.ipynb.\n",
      "Converted 05_database.ipynb.\n",
      "Converted 10_synchro.io.ipynb.\n",
      "Converted 11_synchro.extracting.ipynb.\n",
      "Converted 12_synchro.processing.ipynb.\n",
      "Converted 13_leddome.ipynb.\n",
      "Converted 99_testdata.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
