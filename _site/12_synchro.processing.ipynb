{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp synchro.processing\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# synchro.processing\n",
    "> Processing functions to align stimuli, detect frame timings and correct errors of the display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_thresholds(data):\n",
    "    \"\"\"Function that attempts to get the high and low thresholds. Not working very well\"\"\"\n",
    "    max_val = max(data[len(data)//2:len(data)//2 + 10000000]) #Looking for a max in a portion of the data, from the middle\n",
    "    high_thresh = max_val*3/4 # High threshold set at 3/4th of the max\n",
    "    low_thresh  = max_val*1/4\n",
    "    return low_thresh, high_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from theonerig.synchro.io import *\n",
    "# from theonerig.core import *\n",
    "# from theonerig.utils import *\n",
    "# import matplotlib.pyplot as plt\n",
    "# photodiode_data = load_adc_raw(r\"files/basic_synchro/photodiode_data\", sampling_rate=30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supposidly, get_thresholds should provide low and high threshold for the data, but the low_treshold is a sensitive value that should be checked manually in a record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_thresholds(photodiode_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_first_high(data, threshold):\n",
    "    if np.any(data>threshold):\n",
    "        return np.argmax(data>threshold)\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_first_high` finds the idx of the first frame higher than the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def detect_frames(data, low_threshold, high_threshold, increment, do_reverse=True):\n",
    "    \"\"\"Frame detection (or ON signal detection). Capable of finding frame times produced in a regular\n",
    "    fashion:\n",
    "        - data: raw data\n",
    "        - low_threshold: threshold used to detect begginning of each frame.\n",
    "        - high_threshold: threshold used to assign label to the frames, and used to detect the beggining of the reading frame.\n",
    "        - do_reverse: boolean to indicate if the reverse detection should be done after detecting the first frame.\"\"\"\n",
    "    frame_timepoints, frame_signals = [], []\n",
    "    safe_increment = int(increment*95/100)\n",
    "\n",
    "    first_high = get_first_high(data, high_threshold)\n",
    "    if first_high == -1:\n",
    "        print(\"No high frame detected. Detection can't work.\")\n",
    "        return\n",
    "\n",
    "    frame_timepoints.append(first_high)\n",
    "    frame_signals.append(1)\n",
    "    \n",
    "    if do_reverse:\n",
    "        new_timepoints   = reverse_detection(data, frame_timepoints, low_threshold, increment)\n",
    "        if len(new_timepoints)>1:\n",
    "            new_extrapolated = extend_timepoints(new_timepoints)\n",
    "        else:\n",
    "            new_extrapolated = []\n",
    "        frame_timepoints = new_extrapolated + new_timepoints + frame_timepoints\n",
    "        frame_signals    = [0]*(len(new_timepoints)+len(new_extrapolated)) + frame_signals\n",
    "\n",
    "    i = first_high + safe_increment\n",
    "    while i < len(data):\n",
    "        data_slice = data[i:i+increment//2]\n",
    "        if np.any(data_slice>low_threshold):\n",
    "            i = i+np.argmax(data_slice>low_threshold)\n",
    "        else:\n",
    "            break #This frame sequence is over. Pass the next sequence through this function if there are frames left\n",
    "        frame_timepoints.append(i)\n",
    "        frame_signals.append(int(np.any(data_slice > high_threshold)))\n",
    "        i += safe_increment\n",
    "\n",
    "    frame_timepoints = np.array(frame_timepoints)\n",
    "    frame_signals    = np.array(frame_signals)\n",
    "    frame_timepoints = frame_timepoints - 3 # A slight shift of the timepoints \n",
    "                                            # to include the begginning of the peaks.\n",
    "        \n",
    "    error_check(frame_timepoints)\n",
    "\n",
    "    return frame_timepoints, frame_signals\n",
    "\n",
    "def reverse_detection(data, frame_timepoints, low_threshold, increment):\n",
    "    \"\"\"Detect frames in the left direction.\"\"\"\n",
    "    new_timepoints = []\n",
    "    new_signals = []\n",
    "\n",
    "    safe_increment = int(increment * 105/100)\n",
    "\n",
    "    i = frame_timepoints[0]-safe_increment\n",
    "    while i>0:\n",
    "        data_slice = data[i:i+increment//2]\n",
    "        if np.any(data_slice > low_threshold):\n",
    "            i = i+np.argmax(data_slice > low_threshold)\n",
    "        else:\n",
    "            break #No low threshold crossing found -> no more frames to detect\n",
    "        new_timepoints.append(i)\n",
    "        i-= safe_increment #We move backward of almost a frame\n",
    "\n",
    "    return new_timepoints[::-1]\n",
    "\n",
    "def extend_timepoints(frame_timepoints, n=10):\n",
    "    \"\"\"Extrapolates points to the left. Not really needed now except for the signals idx that would change\n",
    "    otherwise (and some starting index were set manually)\"\"\"\n",
    "    frame_timepoints = np.array(frame_timepoints)\n",
    "    typical_distance = int(np.mean(np.diff(frame_timepoints)))\n",
    "    extended_tp = [frame_timepoints[0]-(i+1)*typical_distance for i in range(n) if (frame_timepoints[0]-(i+1)*typical_distance)>0]\n",
    "    return extended_tp[::-1]\n",
    "\n",
    "def error_check(frame_tp):\n",
    "    \"\"\"Search error by looking at the time between each frame. \n",
    "    DLP is regular and odd time reveal misdetections.\"\"\"\n",
    "    deriv_frame_tp = np.diff(frame_tp)\n",
    "    error_len_th = np.mean(deriv_frame_tp)+np.std(deriv_frame_tp)*6\n",
    "\n",
    "    error_frames = np.abs(deriv_frame_tp)>error_len_th\n",
    "    if np.any(error_frames):\n",
    "        print(\"Error in timepoints detected in frames\", np.where(error_frames)[0], \n",
    "              \"at timepoint\", frame_tp[np.where(error_frames)[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect_frames do frame detection. Works for camera pulses and photodiode data emitted by a DLP. It does it by:\n",
    "* Finding the first frame higher than a threshold\n",
    "* Detecting the frames before if flag do_reverse is set to True\n",
    "* Detect frames\n",
    "* Assign a binary value of if it's higher than the high threshold\n",
    "* Do a quick check on the frames to spot weird results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame_timepoints, frame_signals = detect_frames(photodiode_data, 400, 5000, increment=500)\n",
    "# plt.figure()\n",
    "# plt.plot(photodiode_data)\n",
    "# plt.scatter(frame_timepoints, frame_signals*6000+500, c=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cluster_frame_signals(data, frame_timepoints, n_cluster=5):\n",
    "    \"\"\"Cluster the `frame_timepoints` in `n_cluster` categories depending on the area under the curve.\n",
    "        - data: raw data used to compute the AUC\n",
    "        - frame_timepoints: timepoints delimitating each frame\n",
    "        - n_cluster: Number of cluster for the frame signals\"\"\"\n",
    "    frame_aucs = np.fromiter(map(np.trapz, np.split(data, frame_timepoints)), float)\n",
    "    if frame_timepoints[0] != 0: #We need to remove the first part if it wasn't a full frame\n",
    "        frame_aucs = frame_aucs[1:]\n",
    "    frame_auc_sorted = np.sort(frame_aucs)\n",
    "    deriv = np.array(frame_auc_sorted[1:]-frame_auc_sorted[:-1])\n",
    "    deriv[:5]  = 0 #removing tails values that can show weird stuff\n",
    "    deriv[-5:] = 0\n",
    "    threshold_peak = np.std(deriv)*3\n",
    "    n          = n_cluster - 1\n",
    "    idx_gaps = np.zeros(n+3, dtype=\"int\")\n",
    "    tmp_deriv = deriv.copy()\n",
    "    zero_set_range = 10#int(len(deriv)*0.05) #Around the peaks, we set the values to 0 around\n",
    "    for i in range(n+3): #Detecting more peaks than needed and then taking them starting on the right\n",
    "        if tmp_deriv[np.argmax(tmp_deriv)] < threshold_peak:\n",
    "            if i<n_cluster-1:\n",
    "                print(\"Less transition in AUC detected than needed, results will be weird\")\n",
    "            break\n",
    "        idx_gaps[i] = np.argmax(tmp_deriv)\n",
    "        tmp_deriv[idx_gaps[i]-zero_set_range:idx_gaps[i]+zero_set_range] = 0\n",
    "    idx_gaps = np.sort(idx_gaps)\n",
    "    idx_gaps = idx_gaps[-(n_cluster-1):]\n",
    "    thresholds = np.zeros(n, dtype=\"float\")\n",
    "    for i, idx in enumerate(idx_gaps):\n",
    "        thresholds[i] = (frame_auc_sorted[idx+1] + frame_auc_sorted[idx])/2\n",
    "\n",
    "    return np.array([np.sum(auc>thresholds) for auc in frame_aucs], dtype=int)\n",
    "\n",
    "def cluster_by_epochs(data, frame_timepoints, frame_signals, epochs):\n",
    "    \"\"\"Does the same thing as `cluster_frame_signals`, but working on epochs around which the\n",
    "    number of cluster can differ. Useful when a record contains stimuli with different signals sizes.\"\"\"\n",
    "\n",
    "    frame_aucs = np.fromiter(map(np.trapz, np.split(data, frame_timepoints)), float)\n",
    "    if frame_timepoints[0] != 0: #We need to remove the first part if it wasn't a full frame\n",
    "        frame_aucs = frame_aucs[1:]\n",
    "\n",
    "    max_cluster = max([nclust-1 for (_,_,nclust) in epochs])\n",
    "    \n",
    "    for start,stop,n_cluster in epochs:\n",
    "        n          = n_cluster - 1\n",
    "        norm_clust = max_cluster/n\n",
    "        frame_auc_sorted = np.sort(frame_aucs[start:stop])\n",
    "        deriv = np.array(frame_auc_sorted[1:]-frame_auc_sorted[:-1])\n",
    "        deriv[:5]  = 0 #removing tails values that can show weird stuff\n",
    "        deriv[-5:] = 0\n",
    "        idx_gaps   = np.zeros(n, dtype=\"int\")\n",
    "        tmp_deriv = deriv.copy()\n",
    "        zero_set_range = 10\n",
    "        for i in range(n):\n",
    "            idx_gaps[i] = np.argmax(tmp_deriv)\n",
    "            tmp_deriv[idx_gaps[i]-zero_set_range:idx_gaps[i]+zero_set_range] = 0\n",
    "        idx_gaps = np.sort(idx_gaps)\n",
    "        thresholds = np.zeros(n, dtype=\"float\")\n",
    "        for i, idx in enumerate(idx_gaps):\n",
    "            thresholds[i] = (frame_auc_sorted[idx+1] + frame_auc_sorted[idx])/2\n",
    "\n",
    "        frame_signals[start:stop] = np.array([np.sum(auc>thresholds)*norm_clust for auc in frame_aucs[start:stop]], dtype=int)\n",
    "    return frame_signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frame signals are then refined using cluster_frame_signals of the signals to attribute them a value in a defined range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame_signals = cluster_frame_signals(photodiode_data, frame_timepoints, n_cluster=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.plot(photodiode_data[120000:131800])\n",
    "# plt.scatter(frame_timepoints[frame_timepoints>120000]-120000, frame_signals[frame_timepoints>120000]*1000+500, c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the frame detected, we can create our record master, often named reM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref_timepoints, ref_signals = extend_sync_timepoints(frame_timepoints, frame_signals, up_bound=len(photodiode_data))\n",
    "# reM = RecordMaster([(ref_timepoints, ref_signals)])\n",
    "# print(len(reM[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the reM we just created is from a tiny portion of real data. From now one we will use a premade reM generated from the same dataset, in full."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parse_time(time_str, pattern=\"%y%m%d_%H%M%S\"):\n",
    "    \"\"\"Default parser of rhd timestamps. (serve as a template too)\"\"\"\n",
    "    return datetime.datetime.strptime(time_str, pattern)\n",
    "\n",
    "def get_position_estimate(stim_time, record_time, sampling_rate):\n",
    "    \"\"\"Estimate where in the record should a stimulus start, in sample points\"\"\"\n",
    "    if stim_time < record_time:\n",
    "        return -1\n",
    "    else:\n",
    "        return (stim_time - record_time).seconds * sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record_time = parse_time(\"200331_170849\") #Starting time of that example record found on the filename of the record\n",
    "# print(record_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def match_starting_position(frame_timepoints, frame_signals, stim_signals, estimate_start):\n",
    "    stim_matching_len = min(600, np.where(np.diff(stim_signals)!=0)[0][50]) #Way of getting the 50th change in the signals\n",
    "    #But not higher than 600 (correspond to 10s, and is necessary for moving gratings)\n",
    "#     stim_matching_len = 50\n",
    "    idx_estimate = np.argmax(frame_timepoints>estimate_start)\n",
    "    search_slice = slice(max(0, idx_estimate-1000), min(idx_estimate+1000, len(frame_signals)))\n",
    "#     diff_signals = np.diff(frame_signals[search_slice])\n",
    "#     diff_stim    = np.diff(stim_signals[:stim_matching_len])\n",
    "#     return search_slice.start + np.argmax(np.correlate(diff_signals, diff_stim))\n",
    "    return search_slice.start + np.argmax(np.correlate(frame_signals[search_slice], \n",
    "                                                       stim_signals[:stim_matching_len]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "match_starting_position seaks in the record the first frame of a stimulus. We can use functions from theonerig.synchro.extracting to find out the stimuli used in that record, and get their values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from theonerig.synchro.extracting import get_QDSpy_logs, unpack_stim_npy\n",
    "# log = get_QDSpy_logs(\"./files/basic_synchro\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(log.stimuli[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Unpacking the stimulus printed above\n",
    "# unpacked_checkerboard = unpack_stim_npy(\"./files/basic_synchro/stimulus_data\", \"eed21bda540934a428e93897908d049e\")\n",
    "# print(unpacked_checkerboard[0].shape, unpacked_checkerboard[1].shape, unpacked_checkerboard[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_position_estimate can approximately tell us where the stimulus should be to reduce the search time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate_start = get_position_estimate(log.stimuli[2].start_time, record_time, sampling_rate=30000)\n",
    "# print(\"Estimate position in sample points\", estimate_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stim_start_frame = match_starting_position(reM[\"main_tp\"][0], reM[\"signals\"][0], stim_signals=unpacked_checkerboard[1], estimate_start=estimate_start)\n",
    "# print(stim_start_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def display_match(match_position, reference=None, recorded=None, corrected=None, len_line=50):\n",
    "    start, mid, end = 0, len(reference)//2, len(reference)-len_line\n",
    "    for line in [start, mid, end]:\n",
    "        if reference is not None:\n",
    "            print(\"REF [\"+str(line)+\"] \",\" \".join(map(str,map(int, reference[line:line+len_line]))))\n",
    "        if recorded is not None:\n",
    "            print(\"REC [\"+str(line)+\"] \",\" \".join(map(str,map(int, recorded[line+match_position:line+len_line+match_position]))))\n",
    "        if corrected is not None:\n",
    "            print(\"COR [\"+str(line)+\"] \",\" \".join(map(str,map(int, corrected[line:line+len_line]))))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the match we obtain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_match(stim_start_frame, reference=unpacked_checkerboard[1], recorded=reM[\"signals\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a match!! But be sure to check it everytime, as mismatches occurs. Set then stim_start_frame manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def frame_error_correction(signals, unpacked, algo=\"nw\", **kwargs):\n",
    "    \"\"\"Correcting the display stimulus frame values. Shifts are first detected with one of\n",
    "    `shift_detection_conv` or `shift_detection_NW` and applied to the stimulus template. Then single frame\n",
    "    mismatch are detected and corrected.\n",
    "        - signals: true signal values recorded\n",
    "        - unpacked: stimulus tuple (inten,marker,shader)\n",
    "        - algo: algorithm for shift detection among [nw, conv]\n",
    "        - **kwargs: extra parameter for shift detection functions\n",
    "        \n",
    "        returns: stim_tuple_corrected, shift_log, (error_frames_idx, replacement_idx)\"\"\"\n",
    "    \n",
    "    if algo==\"no_shift\":\n",
    "        intensity, marker, shader  = unpacked[0].copy(), unpacked[1].copy(), unpacked[2]\n",
    "        if shader is not None:\n",
    "            shader = shader.copy()\n",
    "        error_frames, replacements = error_frame_matches(signals, marker, range_=5)\n",
    "        shift_log = []\n",
    "    else:\n",
    "        if algo==\"nw\":\n",
    "            shift_log = shift_detection_NW(signals.astype(int), unpacked[1].astype(int), **kwargs)\n",
    "        elif algo==\"conv\":\n",
    "            shift_log = shift_detection_conv(signals.astype(int), unpacked[1].astype(int), range_=5, **kwargs)\n",
    "        intensity, marker, shader = apply_shifts(unpacked, shift_log)\n",
    "        error_frames, replacements = error_frame_matches(signals, marker, range_=5)\n",
    "    if len(error_frames)>0:\n",
    "        intensity[error_frames]    = intensity[replacements]\n",
    "        marker[error_frames]       = marker[replacements]\n",
    "        if shader is not None:\n",
    "            shader[error_frames] = shader[replacements]\n",
    "    return (intensity, marker, shader), shift_log, list(zip(map(int,error_frames), map(int,replacements)))\n",
    "\n",
    "def error_frame_matches(signals, marker, range_):\n",
    "    \"\"\"Find the frames mismatching and finds in the record the closest frame with an identical signal value\"\"\"\n",
    "    error_frames = np.nonzero(signals!=marker)[0]\n",
    "    if len(error_frames)>0:\n",
    "        where_equal = [((np.where(marker[err_id-range_:err_id+(range_+1)] == signals[err_id])[0]) - range_) for err_id in error_frames]\n",
    "    #filtering out the frames where no match was found\n",
    "        tmp    = np.array([[wheq,err] for (wheq, err) in zip(where_equal, error_frames) if len(wheq)>0])\n",
    "        if len(tmp)==0:\n",
    "            replacements = np.empty(shape=(0,), dtype=int)\n",
    "            error_frames = np.empty(shape=(0,), dtype=int)\n",
    "        else:\n",
    "            where_equal  = tmp[:,0]\n",
    "            error_frames = tmp[:,1]\n",
    "\n",
    "        #Choosing among the equal frame signals the one that is the closest\n",
    "            closest_equal = [wheq[(np.abs(wheq)).argmin()] for wheq in where_equal]\n",
    "            error_frames  = np.array(error_frames, dtype=int)\n",
    "            replacements  = error_frames + np.array(closest_equal, dtype=int)\n",
    "    else:\n",
    "        replacements = np.empty(shape=(0,), dtype=int)\n",
    "        error_frames = np.empty(shape=(0,), dtype=int)\n",
    "    \n",
    "    return error_frames, replacements\n",
    "\n",
    "def apply_shifts(unpacked, op_log):\n",
    "    \"\"\"Applies the shifts found by either shift_detection functions\"\"\"\n",
    "    inten, marker, shader = unpacked[0].copy(), unpacked[1].copy(), unpacked[2]\n",
    "    if shader is not None:\n",
    "        shader = shader.copy()\n",
    "    orig_len = len(marker)\n",
    "    for idx, op in op_log:\n",
    "        if op==\"ins\": #We insert a frame\n",
    "            marker = np.insert(marker, idx, marker[idx], axis=0)\n",
    "            inten  = np.insert(inten , idx, inten[idx], axis=0)\n",
    "            if shader is not None:\n",
    "                shader = np.insert(shader, idx, shader[idx], axis=0)\n",
    "        elif op==\"del\": #We concatenate without the deleted frame\n",
    "            marker = np.concatenate((marker[:idx],marker[idx+1:]))\n",
    "            inten  = np.concatenate((inten[:idx],inten[idx+1:]))\n",
    "            if shader is not None:\n",
    "                shader = np.concatenate((shader[:idx],shader[idx+1:]))\n",
    "    marker = marker[:orig_len]\n",
    "    inten  = inten[:orig_len]\n",
    "    if shader is not None:\n",
    "        shader = shader[:orig_len]\n",
    "    return (inten, marker, shader)\n",
    "\n",
    "def shift_detection_conv(signals, marker, range_):\n",
    "    \"\"\"Detect shifts with a convolution method. First look at how far the next closest frame are, and average\n",
    "    it over the record. When the average cross the -1 or 1 threshold, shift the reference accordingly.\"\"\"\n",
    "    marker = marker.copy()\n",
    "    shift_detected = True\n",
    "    shift_log = []\n",
    "    while shift_detected:\n",
    "        error_frames, replacements = error_frame_matches(signals, marker, range_)\n",
    "\n",
    "        all_shifts = np.zeros(len(marker))\n",
    "        all_shifts[error_frames] = replacements-error_frames\n",
    "        all_shifts_conv = np.convolve(all_shifts, [1/20]*20, mode=\"same\") #Averaging the shifts to find consistant shifts\n",
    "\n",
    "        shift_detected = np.any(np.abs(all_shifts_conv)>.5)\n",
    "        if shift_detected: #iF the -.5 threshold is crossed, we insert a \"fake\" frame in the reference and we repeat the operation\n",
    "            change_idx = np.argmax(np.abs(all_shifts_conv)>.5)\n",
    "            if all_shifts_conv[change_idx]>.5:#Need to delete frame in reference\n",
    "                #Need to refine index to make sure we delete a useless frame\n",
    "                start,stop = max(0,change_idx-2), min(len(marker),change_idx+2)\n",
    "                for i in range(start,stop):\n",
    "                    if marker[i] not in signals[start:stop]:\n",
    "                        change_idx = i\n",
    "                        break\n",
    "                shift_log.append([int(change_idx), \"del\"])\n",
    "                marker = np.concatenate((marker[:change_idx], marker[change_idx+1:], [0]))\n",
    "            else:#Need to insert frame in reference\n",
    "                shift_log.append([int(change_idx), \"ins\"])\n",
    "                #inserting a frame and excluding the last frame to keep the references the same length\n",
    "                marker     = np.insert(marker, change_idx, marker[change_idx], axis=0)[:-1] \n",
    "    return shift_log\n",
    "\n",
    "def shift_detection_NW(signals, marker, simmat_basis=[1,-1,-3,-3,-1], insdel=-10, rowside=20):\n",
    "    \"\"\"Memory optimized Needleman-Wunsch algorithm.\n",
    "    Instead of an N*N matrix, it uses a N*(side*2+1) matrix. Indexing goes slightly differently but\n",
    "    result is the same, with far less memory consumption and exection speed scaling better with\n",
    "    size of the sequences to align.\"\"\"\n",
    "    #Setting the similarity matrix\n",
    "    side = rowside\n",
    "    sim_mat = np.empty((len(marker), side*2+1), dtype=\"int32\")\n",
    "    #Setting the errors\n",
    "    insertion_v = insdel #insertions are commons not so high penalty\n",
    "    deletion_v  = insdel #deletions detection happens during periods of confusion but are temporary. High value\n",
    "    error_match = np.array(simmat_basis) #The value for a 0 matching with [0,1,2,3,4]\n",
    "    error_mat = np.empty((len(simmat_basis),len(simmat_basis)))\n",
    "    for i in range(len(simmat_basis)):\n",
    "        error_mat[i] = np.roll(error_match,i)\n",
    "                \n",
    "    #Filling the similarity matrix\n",
    "    sim_mat[0, side] = error_mat[marker[0], signals[0]]\n",
    "    #Initialization: Setting the score of the first few row and first few column cells\n",
    "    for j in range(side+1, side*2+1):\n",
    "        sim_mat[0,j] = sim_mat[0,side] + insertion_v*j\n",
    "    for i in range(1, side+1):\n",
    "        sim_mat[i,side-i] = sim_mat[0,side] + deletion_v*i\n",
    "          \n",
    "    #Corpus: if j is the first cell of the row, the insert score is set super low\n",
    "    #        if j is the last  cell of the row, the delete score is set super low\n",
    "    for i in range(1, sim_mat.shape[0]):\n",
    "        start = max(side-i+1, 0)\n",
    "        stop  = min(side*2+1, side+sim_mat.shape[0]-i)\n",
    "        for j in range(start, stop):\n",
    "            if j==0:#j==start and i>side:\n",
    "                insert = -99999\n",
    "                delete = sim_mat[i-1, j+1] + deletion_v\n",
    "            elif j==side*2:\n",
    "                delete = -99999\n",
    "                insert = sim_mat[i, j-1] + insertion_v\n",
    "            else:\n",
    "                insert = sim_mat[i, j-1] + insertion_v\n",
    "                delete = sim_mat[i-1, j+1] + deletion_v\n",
    "            match  = sim_mat[i-1, j] + error_mat[marker[i], signals[j+i-side]]\n",
    "            sim_mat[i,j] = max(insert,delete,match)\n",
    "            \n",
    "    #Reading the similarity matrix\n",
    "    #In general, it's the same, at the difference that when i decrement, must add 1 to j compared to usual.\n",
    "    i = len(marker)-1\n",
    "    j = side\n",
    "    shift_log = []\n",
    "    while (i > 0 or j>side-i):\n",
    "        if (i > 0 and j>side-i and sim_mat[i,j]==(sim_mat[i-1,j]+error_mat[marker[i], signals[j+i-side]])):\n",
    "            i -= 1\n",
    "        elif(i > 0 and sim_mat[i,j] == sim_mat[i-1,j+1] + deletion_v):\n",
    "            shift_log.insert(0,(j+i-side+1, \"del\")) #Insert the j value for deletion too because all shifts\n",
    "            i-=1                                        #are relative to the signals recorded, unlike normal NW\n",
    "            j+=1\n",
    "        else:\n",
    "            shift_log.insert(0,(j+i-side, \"ins\"))\n",
    "            j-=1\n",
    "                \n",
    "    return shift_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We correct the stimulus values with frame_error_correction and it gives us back the changes it made to keep track of the errors made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signals = reM[\"signals\"][0][stim_start_frame:stim_start_frame+len(unpacked_checkerboard[0])]\n",
    "# corrected_checkerboard, shift_log, error_frames = frame_error_correction(signals, unpacked_checkerboard, algo=\"nw\")\n",
    "# print(shift_log, len(error_frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def chop_stim_edges(first_frame, last_frame, stim_tuple, shift_log, frame_replacement):\n",
    "    \"\"\"Cut out the stimulus parts not containing actual stimulus, and change the idx values of `shift_log`\n",
    "    and `frame_replacement` to match the new indexing.\"\"\"\n",
    "    inten, marker, shader = stim_tuple\n",
    "    if last_frame<0: #Using negative indexing\n",
    "        last_frame = len(marker)+last_frame\n",
    "    inten = inten[first_frame:last_frame]\n",
    "    marker = marker[first_frame:last_frame]\n",
    "    if shader is not None:\n",
    "        shader = shader[first_frame:last_frame]\n",
    "    \n",
    "    shift_log = [(shift[0]-first_frame, shift[1]) for shift in shift_log if shift[0]<last_frame]\n",
    "    frame_replacement = [(fr[0]-first_frame, fr[1]-first_frame) for fr in frame_replacement if fr[0]<last_frame]\n",
    "    \n",
    "    return (inten, marker, shader), shift_log, frame_replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def detect_calcium_frames(scanning_data, epoch_threshold=-8):\n",
    "    \"\"\"Detect the timing of the 2P frames, epoch by epoch over a record.\"\"\"\n",
    "    \n",
    "    #Finds the start of a stack recording\n",
    "    start_set = np.where((scanning_data[1:] > epoch_threshold) & (scanning_data[:-1] < epoch_threshold))[0]\n",
    "    #Finds the end of a stack recording\n",
    "    end_set   = np.where((scanning_data[1:] < epoch_threshold) & (scanning_data[:-1] > epoch_threshold))[0]\n",
    "    #Splits the records into the epochs\n",
    "    list_epoch = np.array_split(scanning_data, np.ravel(list(zip(start_set, end_set))))[1::2]\n",
    "\n",
    "    def detect_peak_sync(epoch):\n",
    "        #Finds the peaks in an epoch. Peaks have strong SNR so this works fine\n",
    "        return signal.find_peaks(epoch, prominence=2)[0]\n",
    "\n",
    "    return [arr + start_set[i] for i, arr in enumerate(list(map(detect_peak_sync, list_epoch)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_utils.ipynb.\n",
      "Converted 02_processing.ipynb.\n",
      "Converted 03_modelling.ipynb.\n",
      "Converted 04_plotting.ipynb.\n",
      "Converted 05_database.ipynb.\n",
      "Converted 10_synchro.io.ipynb.\n",
      "Converted 11_synchro.extracting.ipynb.\n",
      "Converted 12_synchro.processing.ipynb.\n",
      "Converted 13_leddome.ipynb.\n",
      "Converted 99_testdata.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
